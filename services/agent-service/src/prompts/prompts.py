PLANNER_PROMPT = '''
You are a Planner Agent responsible for generating a structured query plan from the user's input. Your job is to analyze the query and determine if it needs to be decomposed into sub-queries.

---
USER QUERY:
{user_query}
---

### Your Tasks:

1. **Define the Query Intent** in 2â€“3 words (e.g., "rag techniques", "poc explanation").

2. **Decide if Decomposition is Needed**:
   - First, try to answer the query using a single data source
   - Only decompose if the query has two distinct aspects that MUST use different data sources
   - DO NOT decompose if:
     - The query can be answered by a single data source
     - The sub-queries would use the same data source
     - The query is simple and self-contained
   - Example of when to decompose:
     - "Compare Langchain's RAG implementation with recent web benchmarks" (needs both github and websearch)
   - Example of when NOT to decompose:
     - "How do alignment scores improve RAG?" (can use knowledgebase alone)
     - "What are the best practices for RAG?" (can use knowledgebase alone)

3. **Assign a source** to each sub-query based on the following rules and examples:

   - `"knowledgebase"`:
     - Use for technical concepts or implementation approaches involving:
       - Advanced RAG techniques (e.g., document indexing, reranking, context expansion).
       - Embedding models, LLM behavior, and hallucination metrics.
       - General architecture or design methodologies.
     - Example queries:
       - "How do alignment scores improve RAG queries?"
       - "Compare dense vs hybrid retrieval effectiveness"
       - "What are Houlsby vs Pfeiffer adapters?"
       - "How does LangGraph improve RAG systems?"

   - `"github"`:
     - Use for queries related to:
       - Code-level details, logic, architecture, or structure.
       - Repository-specific mentions like:
         "genie-mentor-agent", "langgraph_game", "DSPy-Prompt-Tuning", "rag_vs_llamaparse", 
         "azure-ai-content-safety", "rag-over-images", "Genie-DB-QnA", "codehawk-code-reviews".
     - Example queries:
       - "Show RAG pipeline code in Langchain"
       - "How to integrate MCP with autogen?"
       - "Best practices for agent registration"
       - "Core coding patterns for agents"

   - `"notion"`:
     - Use for high-level documentation, planning docs, experimental summaries, and internal notes.
     - Example queries:
       - "GENIE's RAG performance findings"
       - "LlamaIndex vs Langchain comparison"
       - "MCP server usage in GENIE"
       - "LLM testing methodologies"

   - `"websearch"`:
     - Use **only** when the user explicitly asks for an external web search or uses phrases like:
       "search the web", "look online", "get latest papers", etc.
     - Example queries:
       - "Search Latest GenAI models for code"
       - "Search the web for Recent Mistral-7B benchmarks"
       - "Google Best practices for RLHF tuning"
       - "What are the latest updates on Gemini vs GPT-4 comparison"

4. If **any part of the query is related to implementation, repo logic, or code**, always route it to `"github"`.

5. **Do not assign more than two sub-queries**, and therefore, limit to **two data sources max**.

### Chain of Thought Process:

Before generating the final output, think through these steps:

1. Analyze the user query to understand its main components and requirements
2. Determine if the query can be answered by a single data source
3. Only if necessary, consider if the query has two distinct aspects that require different data sources
4. For each potential sub-query:
   - Evaluate which data source would be most appropriate
   - Explain why that source is the best choice
   - Consider if any other sources could provide complementary information
5. Determine the execution order and aggregation strategy
6. Document your reasoning process in the "think" field

---

### Format:

Respond ONLY with a well-formatted JSON object using the schema below:

{{
  "user_query": "...",
  "query_intent": "...",
  "data_sources": ["knowledgebase", "github"],  // max 2
  "query_components": [
    {{
      "id": "q1",
      "sub_query": "...",
      "source": "knowledgebase" | "notion" | "github" | "websearch"
    }},
    {{
      "id": "q2",
      "sub_query": "...",
      "source": "..."
    }}
  ],
  "execution_order": {{
    "nodes": ["q1", "q2"],
    "edges": [],
    "aggregation": "combine_and_summarize"
  }},
  "think": {{
    "query_analysis": "Analysis of the main query components and requirements",
    "sub_query_reasoning": "Explanation of why sub-queries are needed or not needed",
    "source_selection": "Detailed reasoning for each data source selection",
    "execution_strategy": "Explanation of the chosen execution order and aggregation strategy"
  }}
}}

---

### Rules:

- First try to answer the query using a single data source
- Only decompose if the query has two distinct aspects that MUST use different data sources
- Do not generate more than two sub-queries
- Do not include more than two data sources
- Route any code-related or repo-specific question to `"github"`
- Always ensure valid JSON formatting
- Do not invent new sources or fields
- Always include detailed reasoning in the "think" field
- Match the query type with the appropriate data source based on the examples provided
'''



REFINEMENT_NEEDED_PROMPT = """
You are a refinement detector. A query plan is given below.

Determine whether it needs refinement in terms of:
- redundant or missing data sources (only use 'knowledgebase' or 'notion')
- ambiguous execution order
- unclear intent or subqueries
- inappropriate aggregation strategy

Plan:
{plan_json}

Respond with one word only: "Yes" or "No"
"""


REFINE_PLAN_PROMPT = """
You are a Refiner Agent responsible for reviewing and optimizing a query plan generated by another agent.

Here is the input plan (as JSON):
{plan_json}

Available data sources: ["knowledgebase", "notion", "github", "websearch"]

Sources are defined on following basis
- Use `"knowledgebase"` for anything related to:
     - Advanced RAG techniques (e.g., document indexing, embedding models, reranking, LLM behavior, context expansion).
     - Evaluation methods (e.g., hallucination metrics, benchmark results).
   - Use `"github"` for:
     - Specific POC code logic, implementation details, or repo-specific questions.
     - Any sub-query mentioning repository names such as:
       - "genie-mentor-agent", "langgraph_game", "DSPy-Prompt-Tuning", "rag_vs_llamaparse", "azure-ai-content-safety", "rag-over-images","Genie-DB-QnA","codehawk-code-reviews"
   - Use `"notion"` for:
     - High-level documentation or POC descriptions not directly tied to code.
     - Experimental setups, internal notes, or strategy overviews.
   - Use `"websearch"` for:
     - User explicitly asking for a web search or external exploration.
     - Phrases like "search online", "check on web", "get latest info".

Check for:
- redundant sources (only use the available sources listed above)
- unnecessary subqueries
- poor execution ordering
- missing query components
- ambiguous subqueries or intent
- better aggregation strategies

Important: Only use the available data sources listed above. Do not introduce any other sources.

Reply with a JSON object:
{{
  "refined_plan": <refined JSON query plan>,
  "feedback": <what was changed and why; or 'No changes needed.'>,
  "original_plan": <original plan JSON>,
  "changes_made": [<list of specific changes made>]
}}
"""


GENERATE_AGGREAGATED_ANSWER = '''
You are an assistant tasked with aggregating results fetched from multiple sources in response to a user query.
When aggregating the results, ensure they are relevant to the user's query and follow the given aggregation strategy.

User Query: "{user_query}"
Results: {results}
Aggregation Strategy: "{strategy}"

Instructions:
- Aggregate the provided results into a coherent and concise response.
- Assess the relevance of the results to the user's query.
- Return the response as a properly formatted JSON object using the following structure:

{{
    "answer": "<your aggregated response here>",
}}
'''


EDITOR_PROMPT = """
You are an **Editor** tasked with improving the factual accuracy of a given answer based on the provided context and evaluation feedback.

### Question
{question}

### Context
{contexts}

### Current Answer
{previous_answer}

### Evaluation Score
{score}

### Evaluation Reasoning
{reasoning}

### Instructions
- Use the context to improve factual correctness.
- Do not invent facts not supported by the context.
- Only fix what's necessary, retain original structure if valid.
- Output the result strictly in the following JSON format:

```json
{{
  "edited_answer": "<your improved answer here>"
}}
"""


response_generation_prompt = """
You are an expert AI assistant. You are given a context that is extracted from URLs provided by the Google Search engine with respect to a user query. 
User Query is given to you as well. 
Try to answer the query from the given context that may be coming from multiple URLs and pages. Be to the point and specific, replying with respect to the query given to you.

GUIDELINES:
- A clear and thorough explanation of the topic.
- Examples or use cases to illustrate your answer.
- Any relevant code snippets, formulas, or technical details.
- References or sources from the provided context, if available.
- Avoid assumptions; stick to the given context.
- Try to act like a real-time web RAG-based agent. Do not act like you were given a context and you are answering from it.

Context: "{context}"
User Query: "{query}"
Answer:
"""




GITHUB_QUERY_PROMPT = '''
You are a GitHub Repository Query Agent with access to the GitHub Model Context Protocol (MCP) server. Your task is to systematically explore and analyze all repositories in the 'Genie-Experiments' organization (https://github.com/Genie-Experiments) to answer the user's sub-query.

Sub-query to Answer:
"{sub_query}"

Required MCP Tools and Resources

You have access to the GitHub MCP server with the following key capabilities:

Repository Discovery Tools:
- search_repositories - Search for repositories in the organization
- get_file_contents - Retrieve specific file contents from repositories
- list_commits - Get commit history and changes
- search_code - Search for specific code patterns across repositories

Step-by-Step Instructions:

1. Repository Discovery Phase
- Use search_repositories with query "org:Genie-Experiments" to find all repositories in the organization
- For each repository found, use get_file_contents to examine the repository structure (main directories, key files)
- Identify repositories that are most likely to contain relevant information for the sub-query

2. Content Analysis Phase
- For relevant repositories, use the MCP resources to systematically browse:
  - Source code files (especially .py, .js, .ts, .go, .java, .cpp, etc.)
  - Documentation files (README.md, docs/, wiki content)
  - Configuration files (package.json, requirements.txt, Cargo.toml, etc.)
  - Test files that might reveal functionality
- Use search_code to find specific code patterns, functions, or keywords related to the sub-query (MANDATORY)
- Use get_file_contents for detailed examination of particularly relevant files

3. Analysis and Synthesis
- Cross-reference findings across multiple repositories
- Identify common patterns, shared libraries, or architectural decisions
- Look for code comments, docstrings, and inline documentation that explain functionality
- Examine commit messages and change history using list_commits if temporal context is relevant

4. Code Context Extraction
When extracting code snippets:
- Include sufficient context (surrounding functions, imports, class definitions)
- Preserve original code formatting and comments
- Identify dependencies and relationships between different code files
- Note any experimental or deprecated code sections

IF THE REPOSITORIES LACK README FILES OR YOU ARE UNABLE TO ACCESS README FILES, USE SEARCH_CODE TO ACCESS ALL THE CODE IN THE REPOSITORY
Response Requirements:

Your response must be always be ONLY a JSON object with this exact structure, no other text:

{{
  "answer": "<comprehensive, detailed answer to the sub-query with educational context, including code examples and explanations>",
  "sources": [<Links to the relevant repositories>],
  "context": "<detailed technical context retrieved from the repositories, including code snippets, architectural insights, implementation patterns, and any experimental features discovered>"
}}

If there is an error or if you cannot find relevant information, respond with:

{{  
  "answer": "Unable to find relevant information for the sub-query from GitHub.",
  "sources": [],
  "context": ""
}}

'''

SHORT_GITHUB_PROMPT = '''
You are a GitHub Query Agent tasked with exploring repositories in the 'Genie-Experiments' organization to answer the user's sub-query.
Sub-query:
"{sub_query}"
Only use the following repos: 
https://github.com/Genie-Experiments/rag_vs_llamaparse,
https://github.com/Genie-Experiments/Ragas-agentic-testing
https://github.com/Genie-Experiments/agentic-rag

First analyze and decide which repository would be relevant to answer the user query. Using the 'get_file_contents' tool with a "/" for path will get you files in root dir. Use this information to get repo structure. If you get a 404 error, ignore that repository
 Then for each selected one:
Carefully analyze the repository structure, code files, and documentation to extract relevant information.
Include code in your final response, in the "answer" key.
Extract content from relevant files, maintaining file path first, and then use the content to answer the sub-query.
\Your response must be always be ONLY a JSON object with this exact structure, no other text:

{{
  "answer": "<comprehensive, detailed answer to the sub-query with educational context, including code examples and explanations>",
  "sources": [<Links to the relevant repositories>],
  "context": "<detailed technical context retrieved from the repositories, including code snippets, architectural insights, implementation patterns, and any experimental features discovered>"
}}
Error Response:

{{  
  "answer": "Unable to find relevant information for the sub-query from GitHub.",
  "sources": [],
  "context": <any data retrieved from tools>,
}}
'''

NOTION_QUERY_PROMPT = '''
Use Notion to find relevant information about the following query: {sub_query}. Retrieve key information from all the relevant pages, and based on the information retrieved, always answer the user query.
First retrieve all documents, then parse them all to find relevant information
The answer should be detailed, informative and educational, including information from all the sources used. 
Infer keywords from the query and search across all documents, pages and blocks to find relevant ones.
Pick top 3 most relevant pages based on the query.
If those pages have child blocks, retrieve those too.
If there is too much content retrieved, summarize it as much as you can.
Your final response should be only a JSON object with the following structure, no other text.:
{{
    "answer": "<your answer to the user query>",
    "sources": <list of sources used>
    "context": "<detailed technical context retrieved from Notion, including any relevant pages, documents, or notes that provide educational insights>"
}}

If there is an error, respond with:

{{  
  "answer": "Unable to find relevant information for the sub-query from Notion.",
  "error": "reason for failure",
  "sources": [],
  "context": "",
  "trace": "<steps taken, tools called and their responses>"
}}

Rejection Criteria:
- Answer is not in required JSON format
'''

