from pydantic import BaseModel, Field,  HttpUrl
from typing import Any, Dict, List, Literal, Optional


class LLMUsage(BaseModel):
    model: str = Field(..., description="The LLM model used")
    input_tokens: int = Field(..., description="Number of input tokens")
    output_tokens: int = Field(..., description="Number of output tokens")
    total_tokens: int = Field(..., description="Total number of tokens")

class EditorAgentInput(BaseModel):
    question: str = Field(..., description="The original user question")
    previous_answer: str = Field(..., description="The answer that needs to be reviewed or edited")
    score: Optional[float] = Field(None, description="The factual accuracy score of the previous answer")
    reasoning: Optional[str] = Field("", description="The reasoning or explanation behind the score")
   # contexts: List[str] = Field(..., description="List of contextual strings retrieved from source documents")
    contexts: Dict[str, List[str]]

class EditorAgentOutput(BaseModel):
    answer: str = Field(..., description="The revised answer generated by the EditorAgent")
    error: Optional[str] = Field(None, description="Error message if an exception occurred during processing")
    llm_usage: Optional[LLMUsage] = Field(None, description="Token usage information for the LLM call")


class EvalAgentInput(BaseModel):
    question: str = Field(..., description="The question asked to the model")
    answer: str = Field(..., description="The answer provided by the model that needs to be evaluated")
    contexts:  List[str] = Field(..., description="List of lists of contextual strings used to support the answer")

class FactEvaluation(BaseModel):
    fact: str = Field(..., description="Individual fact extracted from the answer")
    label: Literal["yes", "no", "unclear"] = Field(..., description="Judgment on the factuality of the fact")
    reasoning: str = Field(..., description="Justification for the assigned label")


class EvalAgentOutput(BaseModel):
    score: float = Field(..., description="Final score computed as ratio of 'yes' labels to total facts")
    reasoning: List[dict] = Field(..., description="List of fact evaluation dicts")
    error: Optional[str] = Field(None, description="Error message if evaluation failed")
    llm_usage: Optional[LLMUsage] = Field(None, description="Token usage information for the LLM call")
    execution_time_ms: Optional[int] = Field(None, description="Total execution time for evaluation in milliseconds")


class CompletenessCheckOutput(BaseModel):
    is_complete: bool = Field(..., description="Whether the answer is complete according to the evaluation")
    reasoning: str = Field(..., description="Explanation for the completeness assessment")


class KBMetadata(BaseModel):
    title: Optional[str]
    source: Optional[str]
    page: Optional[int]
    document_title: Optional[str] = None  

class KBResponse(BaseModel):
    answer: str
    sources: List[str]
    metadata: List[Dict[str, Any]]
    error: Optional[str] = None
    num_hops: int = 0
    trace: List[Dict[str, Any]] = []



class WebSearchMetadata(BaseModel):
    title: Optional[str]
    url: Optional[HttpUrl]
    description: Optional[str]

class WebSearchResponse(BaseModel):
    answer: str
    sources: List[str]
    metadata: List[WebSearchMetadata]
    error: Optional[str]

class WorkbenchResponse(BaseModel):
    answer: str
    sources: List[str]
    metadata: List
    error: Optional[str]
