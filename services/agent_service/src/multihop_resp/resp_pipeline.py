
# services/agent_service/src/multihop_resp/resp_pipeline.py

import os
from dotenv import load_dotenv
load_dotenv()
from .prompts import JUDGE_PROMPT, PLAN_PROMPT, SUMMARIZER_PROMPT_GLOBAL, SUMMARIZER_PROMPT_LOCAL, GENERATOR_PROMPT, GLOBAL_SUMMARIZER_PROMPT, LOCAL_SUMMARIZER_PROMPT, PLANNER_REASONER_PROMPT, GENIE_DOCS_TOC
from .utils import get_chroma_retriever, retrieve_docs
from langchain_groq import ChatGroq
from langchain_community.embeddings import HuggingFaceEmbeddings
# Use the colored logger from utils.logging
from ..utils.logging import setup_logger, get_logger
setup_logger()
logger = get_logger("ReSPPipeline")

from ..prompts.kb_agent_prompt import kb_assistant_prompt
from langchain_community.vectorstores import Chroma
from ..source_agents.knowledgebase_agent import rerank, boost_by_metadata
import json, re

def parse_reasoner_json(text: str) -> dict:
    """
    Robustly extract the first {...} JSON object appearing in `text`.
    Returns {} if nothing parsable is found.
    """
    match = re.search(r'\{.*\}', text, flags=re.S)
    if not match:
        return {}
    try:
        return json.loads(match.group(0))
    except Exception:
        return {}


# For local testing, set CHROMA_DB_PATH directly
CHROMA_DB_PATH = r"C:\Users\Emumba\Downloads\Projects\genie-mentor-agent\services\genie-kbdocs-v1-thurs\chroma_db"

db = Chroma(persist_directory=CHROMA_DB_PATH, embedding_function=None)
print("Number of documents/chunks in the database:", db._collection.count())  # Should print the number of documents/chunks

class ReSPPipeline:
    def __init__(
        self,
        persist_directory,
        groq_api_key,
        model_name="meta-llama/llama-4-maverick-17b-128e-instruct",
        embedding_model_name="BAAI/bge-small-en-v1.5",
        retrieval_k=15
    ):
        self.embedding_model = HuggingFaceEmbeddings(model_name=embedding_model_name)
        self.retriever = get_chroma_retriever(persist_directory, self.embedding_model, k=retrieval_k)
        self.llm = ChatGroq(temperature=0.1, groq_api_key=groq_api_key, model_name=model_name)
        self.retrieval_k = retrieval_k

    def run(self, main_question, max_hops=5):
        global_memory = []
        local_memory = []
        all_sub_questions = []  # Flat list of all sub-questions ever asked
        hops_trace = []
        hop = 0
        # --- First hop: always answer the main question ---
        hop += 1
        hop_info = {"hop": hop, "sub_questions": []}
        # Retrieve and summarize for main question
        docs = retrieve_docs(main_question, self.retriever)
        docs = rerank(main_question, docs)
        docs = boost_by_metadata(main_question, docs)
        docs = docs[:5]
        doc_texts = [f"[Metadata: {', '.join(f'{k}: {v}' for k, v in doc.metadata.items())}]\n{doc.page_content}" for doc in docs]
        global_summary_prompt = GLOBAL_SUMMARIZER_PROMPT.format(
            main_question=main_question,
            docs="\n".join(doc_texts)
        )
        global_summary = self.llm.invoke(global_summary_prompt).content
        local_summary_prompt = LOCAL_SUMMARIZER_PROMPT.format(
            sub_question=main_question,
            docs="\n".join(doc_texts)
        )
        local_summary = self.llm.invoke(local_summary_prompt).content
        global_memory.append(global_summary)
        local_memory.append({"sub_question": main_question, "response": local_summary})
        hop_info["sub_questions"].append({
            "sub_question": main_question,
            "retrieved_docs": [
                {"content": doc.page_content, "metadata": dict(doc.metadata)} for doc in docs
            ],
            "global_summary": global_summary,
            "local_summary": local_summary
        })
        # --- Now call planner to get next_sub_questions ---
        planner_reasoner_prompt = PLANNER_REASONER_PROMPT.format(
            main_question=main_question,
            global_memory="\n".join(global_memory),
            local_memory="\n".join([f"Sub-question: {m['sub_question']}\nResponse: {m['response']}" for m in local_memory]),
            genie_docs_toc=GENIE_DOCS_TOC,
            previous_sub_questions=""
        )
        logger.debug(f"[Planner Prompt Debug] {planner_reasoner_prompt}")
        reasoner_raw = self.llm.invoke(planner_reasoner_prompt).content
        logger.info(f"[Hop 1] Reasoner raw: {reasoner_raw}")
        reasoner = parse_reasoner_json(reasoner_raw)
        hop_info["reasoner_output"] = reasoner
        hops_trace.append(hop_info)
        # --- Use 'next_sub_questions' as the key for sub-questions ---
        current_sub_questions = reasoner.get("next_sub_questions", [])
        if not current_sub_questions:
            sq = reasoner.get("next_sub_question")
            if sq:
                current_sub_questions = [sq]
            else:
                logger.warning("[Hop 1] No sub-questions generated by planner. Stopping.")
                return {"answer": None, "trace": hops_trace, "num_hops": 1}
        # Ensure all_sub_questions only contains strings
        for sq in current_sub_questions:
            if isinstance(sq, dict):
                all_sub_questions.append(sq.get("sub_question", ""))
            else:
                all_sub_questions.append(sq)
        sufficient = reasoner.get("sufficient", False)
        while hop < max_hops and not sufficient:
            hop += 1
            hop_info = {"hop": hop, "sub_questions": []}
            subq_results = []
            for subq in current_sub_questions:
                # Ensure subq is a string (handle dicts with 'sub_question' key)
                if isinstance(subq, dict):
                    query_text = subq.get("sub_question", "")
                else:
                    query_text = subq
                docs = retrieve_docs(query_text, self.retriever)
                docs = rerank(query_text, docs)
                docs = boost_by_metadata(query_text, docs)
                docs = docs[:5]
                doc_texts = [f"[Metadata: {', '.join(f'{k}: {v}' for k, v in doc.metadata.items())}]\n{doc.page_content}" for doc in docs]
                global_summary_prompt = GLOBAL_SUMMARIZER_PROMPT.format(
                    main_question=main_question,
                    docs="\n".join(doc_texts)
                )
                global_summary = self.llm.invoke(global_summary_prompt).content
                local_summary_prompt = LOCAL_SUMMARIZER_PROMPT.format(
                    sub_question=query_text,
                    docs="\n".join(doc_texts)
                )
                local_summary = self.llm.invoke(local_summary_prompt).content
                global_memory.append(global_summary)
                local_memory.append({"sub_question": query_text, "response": local_summary})
                subq_results.append({
                    "sub_question": query_text,
                    "retrieved_docs": [
                        {"content": doc.page_content, "metadata": dict(doc.metadata)} for doc in docs
                    ],
                    "global_summary": global_summary,
                    "local_summary": local_summary
                })
            # When joining previous_sub_questions, ensure only strings
            previous_sub_questions_str = "\n".join(
                sq.get("sub_question", "") if isinstance(sq, dict) else sq
                for sq in all_sub_questions
            )
            planner_reasoner_prompt = PLANNER_REASONER_PROMPT.format(
                main_question=main_question,
                global_memory="\n".join(global_memory),
                local_memory="\n".join([f"Sub-question: {m['sub_question']}\nResponse: {m['response']}" for m in local_memory]),
                genie_docs_toc=GENIE_DOCS_TOC,
                previous_sub_questions=previous_sub_questions_str
            )
            logger.debug(f"[Planner Prompt Debug] {planner_reasoner_prompt}")
            reasoner_raw = self.llm.invoke(planner_reasoner_prompt).content
            logger.info(f"[Hop {hop}] Reasoner raw: {reasoner_raw}")
            reasoner = parse_reasoner_json(reasoner_raw)
            hop_info["sub_questions"] = subq_results
            hop_info["reasoner_output"] = reasoner
            hops_trace.append(hop_info)
            sufficient = reasoner.get("sufficient", False)
            if sufficient:
                break
            next_subqs = reasoner.get("next_sub_questions", [])
            if not next_subqs:
                sq = reasoner.get("next_sub_question")
                if sq:
                    next_subqs = [sq]
            # Ensure next_subqs are strings
            next_subqs = [q.get("sub_question", "") if isinstance(q, dict) else q for q in next_subqs if q]
            next_subqs = [q for q in next_subqs if q and q not in all_sub_questions]
            if not next_subqs:
                logger.warning(f"[Hop {hop}] No new sub-questions found. Stopping.")
                break
            for q in next_subqs:
                all_sub_questions.append(q)
            current_sub_questions = next_subqs
        # --- Final answer ---
        logger.info("[ReSPPipeline] Generating final answer from all accumulated evidence.")
        combined_memory = "\n".join(global_memory + [f"{m['sub_question']}\n{m['response']}" for m in local_memory])
        logger.info(f"[ReSPPipeline] main_question passed to generator_prompt: {main_question}")
        generator_prompt = GENERATOR_PROMPT.format(
                combined_memory=combined_memory, main_question=main_question
        )
        answer = self.llm.invoke(generator_prompt).content
        hops_trace.append({
            "hop": "final",
            "generator": answer,
            "global_memory": list(global_memory),
            "local_memory": list(local_memory)
        })
        num_real_hops = sum(1 for h in hops_trace if isinstance(h.get("hop"), int))
        return {"answer": answer, "trace": hops_trace, "num_hops": num_real_hops}

if __name__ == "__main__":
    persist_dir = CHROMA_DB_PATH
    groq_api_key = os.environ["GROQ_API_KEY"]
    pipeline = ReSPPipeline(
        persist_dir,
        groq_api_key,
        model_name="meta-llama/llama-4-maverick-17b-128e-instruct",
        retrieval_k=10
    )
    question = input("Enter your question: ")
    result = pipeline.run(question)
    print(json.dumps(result, indent=2, ensure_ascii=False))