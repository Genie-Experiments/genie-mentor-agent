[
  {
    "text": "Sentence embeddings enable us to capture the semantic similarity of short texts. Most sentence embedding models are trained for general semantic textual similarity tasks. Therefore, to use sentence embeddings in a particular domain, the model must be adapted to it in order to achieve good results. Usually, this is done by fine-tuning the entire sentence embedding model for the domain of interest. While this approach yields state-of-the-art results, all of the model’s weights are updated during finetuning, making this method resource-intensive. Therefore, instead of fine-tuning entire sentence embedding models for each target domain individually, we propose to train lightweight adapters. These domain-specific adapters do not require fine-tuning all underlying sentence embedding model parameters. Instead, we only train a small number of additional parameters while keeping the weights of the underlying sentence embedding model fixed. Training domain-specific adapters allows always using the same base model and only exchanging the domain-specific adapters to adapt sentence embeddings to a specific domain. We show that using adapters for parameter-efficient domain adaptation of sentence embeddings yields competitive performance within 1% of a domainadapted, entirely fine-tuned sentence embedding model while only training approximately 3.6% of the parameters.",
    "metadata": {
      "page_number": null,
      "header": "Abstract",
      "chunk_index": 1,
      "file_path": "Efficient Domain Adaptation of Sentence Embeddings Using Adapters.pdf",
      "token_count": 1375
    }
  },
  {
    "text": "Learning sentence embeddings is an essential task in natural language processing ( NLP ) and has already been extensively investigated in the literature (Kiros et al., 2015; Hill et al., 2016; Conneau et al., 2017; Logeswaran and Lee, 2018; Cer et al., 2018; Reimers and Gurevych, 2019; Gao et al., 2021; Wu et al., 2022; Schopf et al., 2023d,a). Sentence embeddings are especially useful in information retrieval (Lewis et al., 2020; Schopf et al., 2022; Figure 1: Sentence embedding models are usually trained to obtain state-of-the-art sentence representations for general semantic textual similarity tasks. By injecting domain-specific knowledge of adapters into the sentence embedding model, we can efficiently adapt the resulting representations for semantic textual similarity tasks in different domains. Schneider et al., 2022) or unsupervised text classification settings (Schopf et al., 2021, 2023b,c) Lately, the most popular approach for sentence embedding learning is to fine-tune pretrained language models with a contrastive learning objective (Liu et al., 2021; Zhang et al., 2022; Chuang et al., 2022; Nishikawa et al., 2022; Cao et al., 2022; Jiang et al., 2022). While this approach provides state-of-theart results, all of the model’s weights are updated during fine-tuning, making this method resourceintensive. This is a problem, particularly when domain-specific models are needed. Then, a specialized model must be trained for each domain of interest, resulting in resource-intensive training. Recently, _adapters_ have emerged as a parameterefficient strategy to fine-tune Language Models ( LM s). Adapters do not require fine-tuning of all parameters of the pretrained model and instead introduce a small number of task-specific parameters while keeping the underlying pretrained lan General Embedding Sentence Sentence guage model fixed (Pfeiffer et al., 2021a). They enable efficient parameter sharing between tasks and domains by training many task-specific, domainspecific, and language-specific adapters for the same model, which can be exchanged and combined post-hoc (Pfeiffer et al., 2020a). Therefore, many different adapter architectures have been proposed for various domains and tasks (Pfeiffer et al., 2020b, 2021b; Vidoni et al., 2020; He et al., 2021; Le et al., 2021; Parovic et al. ´, 2022; Lee et al., 2022). However, to the best of our knowledge, no method currently exists for efficient domain adaptation of sentence embeddings using adapters. In this paper, we aim to bridge this gap by proposing approaches for adapter-based domain adaptation of sentence embeddings, allowing us to train models for many different domains efficiently. Therefore, we investigate how to adapt general pretrained sentence embedding models to different domains using domain-specific adapters. As shown in Figure 1, this allows always using the same base model to adapt sentence embeddings to a specific domain and only needing to exchange the domain-specific adapters. Accordingly, we train lightweight adapters for each domain and avoid expensive training of entire sentence embedding models.",
    "metadata": {
      "page_number": null,
      "header": "1 Introduction",
      "chunk_index": 2,
      "file_path": "Efficient Domain Adaptation of Sentence Embeddings Using Adapters.pdf",
      "token_count": 3120
    }
  },
  {
    "text": "Adapters have been introduced by Houlsby et al. (2019) as a parameter-efficient alternative for taskspecific fine-tuning of language models. Since their introduction, adapters have been used to fine-tune models for single tasks as well as in multi-task settings (Pfeiffer et al., 2021a). Usually, adapters are used to solve tasks such as classification (Lauscher et al., 2020), machine translation (Baziotis et al., 2022), question answering (Pfeiffer et al., 2022), or reasoning (Pfeiffer et al., 2021a). While there exist adapters for semantic textual similarity ( STS ) tasks on the _AdapterHub_ (Pfeiffer et al., 2020a), these are trained on general STS datasets using a task-unspecific pretrained language model as a basis. We, however, focus on adapting pretrained sentence embedding models to specific domains using adapters.",
    "metadata": {
      "page_number": 2,
      "header": "2 Related Work",
      "chunk_index": 3,
      "file_path": "Efficient Domain Adaptation of Sentence Embeddings Using Adapters.pdf",
      "token_count": 832
    }
  },
  {
    "text": "We assume we have a base sentence embedding model from the source domain and labeled datasets for each target domain. Instead of fine-tuning the entire sentence embedding model for each target domain individually, we train lightweight adapters for each domain. This domain-specific fine-tuning with adapters involves adding a small number of new parameters to the sentence embedding model During training, the parameters of the sentence embedding model are frozen, and only the weights of the adapters are updated. Formally, we adopt the general definition for adapter-based fine-tuning of Pfeiffer et al. (2021a) as follows: For each of the _N_ domains, the sentence embedding model is initialized with parameters Θ 0 Additionally, a set of new and randomly initialized adapter parameters Φ _n_ are introduced. The parameters Θ 0 are fixed and only the parameters Φ _n_ are trained. Given training data _D_ _n_ and a loss function _L_, the objective for each domain _n ∈_ 1 _, ..., N_ is of the form: Φ _n_ _←_ argmin _L_ ( _D_ _n_ ; Θ 0 _,_ Φ) (1) Φ Usually, the adapter parameters Φ _n_ are significantly less than the parameters Θ 0 of the base model (Pfeiffer et al., 2021a), e.g., only 3.6% of the parameters of the pretrained model in Houlsby et al. (2019).",
    "metadata": {
      "page_number": 2,
      "header": "3 Method",
      "chunk_index": 4,
      "file_path": "Efficient Domain Adaptation of Sentence Embeddings Using Adapters.pdf",
      "token_count": 1264
    }
  },
  {
    "text": "In this section, we describe the used adapter architectures, loss functions, and datasets. In all experiments, we use SimCSE _sup−bert−base_ (Gao et al., 2021) as the base sentence embedding model It is trained on natural language inference ( NLI ) datasets (Bowman et al., 2015; Williams et al., 2018) for STS tasks in the general domain. We fine-tune all models and adapters for five epochs using a learning rate of 1 _e_ _[−]_ [5] .",
    "metadata": {
      "page_number": 2,
      "header": "4 Experiments",
      "chunk_index": 5,
      "file_path": "Efficient Domain Adaptation of Sentence Embeddings Using Adapters.pdf",
      "token_count": 435
    }
  },
  {
    "text": "This adapter, introduced by Houlsby et al. (2019), uses a bottleneck architecture. The adapter modules are added after both the multi-head attention and feed-forward block in each transformer layer (Vaswani et al., 2017) of the base model. The adapter layers transform their input into a very low-dimensional representation and upsample it again to the same dimension in the output. This generates a parameter-efficient lowerdimensional representation while most information is kept. layers containing _N_ transformer layers and two projection layers across which a skip connection is applied. The adapter layers combine the output ~~of an intermediate transformer layer in~~ the base model with the output of a previous adapter layer To generate the final output, the last hidden states of the adapter are concatenated with the last hidden states of the base model and transformed into the correct output dimension with a simple dense layer Adapter Layer + Feedforward up-project Nonlinearity Feedforward down-project Figure 2: Houlsby-Adapter architecture as introduced by Houlsby et al. (2019). On the left side, the adapter is illustrated to be added twice to each transformer layer. Once after the multi-head attention and once after the feed-forward layer. On the right side, the bottleneck architecture of the adapter is presented.",
    "metadata": {
      "page_number": 2,
      "header": "Houlsby-Adapter",
      "chunk_index": 6,
      "file_path": "Efficient Domain Adaptation of Sentence Embeddings Using Adapters.pdf",
      "token_count": 1338
    }
  },
  {
    "text": "This adapter, introduced by Pfeiffer et al. (2021a), also uses a bottleneck architecture. However, the adapter modules are added only after the feed-forward block in each transformer layer of the base model. This architecture allows merging multiple adapters trained on different tasks. In this work, however, this multitask learning capability is not needed, and we only use the single-task mode. |Col1|BERT-base| |---|---| |~~No. of Parameters~~<br>**Base Model**<br>|110M| |~~No. of Parameters~~<br>**Houlsby-Adapter**<br>|4M| |~~No. of Parameters~~<br>**Pfeiffer-Adapter**<br>|10M| |~~No. of Parameters~~<br>**K-Adapter**|47M| Figure 3: Pfeiffer-Adapter architecture as introduced by Pfeiffer et al. (2021a). Unlike the Houlsby-Adapter, a single Pfeiffer-Adapter is added in each transformer block only after the forward layer.",
    "metadata": {
      "page_number": 2,
      "header": "Pfeiffer-Adapter",
      "chunk_index": 7,
      "file_path": "Efficient Domain Adaptation of Sentence Embeddings Using Adapters.pdf",
      "token_count": 831
    }
  },
  {
    "text": "This adapter, introduced by Wang et al. (2021), works as outside plug-in for the base model. Each adapter model consists of _K_ adapter Figure 4: K-Adapter architecture as introduced by Wang et al. (2021). The adapter layer (left) consists of two projection layers, _N_ = 2 transformer layers, and a skip connection between two projection layers. The adapter layers are plugged among different transformer layers of the base model. The final output consists of the concatenated last hidden states of the adapter and the base model. For reference, Table 1 shows the number of parameters per adapter model compared to commonly used base models, highlighting the efficient nature of adapters.",
    "metadata": {
      "page_number": 2,
      "header": "K-Adapter",
      "chunk_index": 8,
      "file_path": "Efficient Domain Adaptation of Sentence Embeddings Using Adapters.pdf",
      "token_count": 689
    }
  },
  {
    "text": "We investigate two different loss functions that are proven to teach models to learn a notion of STS from triplets of examples. We assume a set of triplets _D_ = _{_ ( _x_ _i_ _, x_ [+] _i_ _[, x]_ _[−]_ _i_ [)] _[}]_ [, where] _[ x]_ _[i]_ [ is an an-] chor ~~sentence,~~ ~~_x_~~ [+] _i_ [is a positive sample and] _[ x]_ _[−]_ _i_ [i] ~~[s a]~~ negative sample. With _h_ _i_, _h_ [+] _i_ [, and] _[ h]_ _[−]_ _i_ [as represen-]",
    "metadata": {
      "page_number": 3,
      "header": "4.2 Loss Functions",
      "chunk_index": 9,
      "file_path": "Efficient Domain Adaptation of Sentence Embeddings Using Adapters.pdf",
      "token_count": 429
    }
  },
  {
    "text": "_→_ **AskUbuntu** **SciDocs** **Average** |→ Models ↓|Col2|Cite CC CR CV|Col4| |---|---|---|---| |_Out-of-the-box_ SimCSE_ (lower bound)_|60.3|79.3<br>82.10<br>76.87<br>78.36|75.39| |_ℓ_1<br>Houlsby-Adapter<br>64.0<br>**88.2**<br>88.69<br>**82.42**<br>83.99<br>81.46<br>Pfeiffer-Adapter<br>63.8<br>87.8<br>88.73<br>81.65<br>83.27<br>81.05<br>K-Adapter<br>62.5<br>85.6<br>87.70<br>80.09<br>82.85<br>79.75<br>_In-domain supervised_ SimCSE_ (upper bound)_<br>65.3<br>88.0<br>87.74<br>84.15<br>83.32<br>81.70|64.0<br>63.8<br>62.5|**88.2**<br>88.69<br>**82.42**<br>83.99<br>87.8<br>88.73<br>81.65<br>83.27<br>85.6<br>87.70<br>80.09<br>82.85|**88.2**<br>88.69<br>**82.42**<br>83.99<br>87.8<br>88.73<br>81.65<br>83.27<br>85.6<br>87.70<br>80.09<br>82.85| |_ℓ_1<br>Houlsby-Adapter<br>64.0<br>**88.2**<br>88.69<br>**82.42**<br>83.99<br>81.46<br>Pfeiffer-Adapter<br>63.8<br>87.8<br>88.73<br>81.65<br>83.27<br>81.05<br>K-Adapter<br>62.5<br>85.6<br>87.70<br>80.09<br>82.85<br>79.75<br>_In-domain supervised_ SimCSE_ (upper bound)_<br>65.3<br>88.0<br>87.74<br>84.15<br>83.32<br>81.70|65.3|88.0<br>87.74<br>84.15<br>83.32|88.0<br>87.74<br>84.15<br>83.32| |_ℓ_2<br>Houlsby-Adapter<br>**64.5**<br>87.3<br>**89.01**<br>82.41<br>**84.42**<br>**81.53**<br>Pfeiffer-Adapter<br>64.2<br>87.0<br>88.63<br>81.98<br>84.41<br>81.24<br>K-Adapter<br>62.8<br>85.3<br>87.92<br>80.05<br>83.29<br>79.87<br>_In-domain supervised_ SimCSE_ (upper bound)_<br>65.2<br>88.3<br>88.11<br>84.46<br>83.63<br>81.94|**64.5**<br>64.2<br>62.8|87.3<br>**89.01**<br>82.41<br>**84.42**<br>87.0<br>88.63<br>81.98<br>84.41<br>85.3<br>87.92<br>80.05<br>83.29|87.3<br>**89.01**<br>82.41<br>**84.42**<br>87.0<br>88.63<br>81.98<br>84.41<br>85.3<br>87.92<br>80.05<br>83.29| |_ℓ_2<br>Houlsby-Adapter<br>**64.5**<br>87.3<br>**89.01**<br>82.41<br>**84.42**<br>**81.53**<br>Pfeiffer-Adapter<br>64.2<br>87.0<br>88.63<br>81.98<br>84.41<br>81.24<br>K-Adapter<br>62.8<br>85.3<br>87.92<br>80.05<br>83.29<br>79.87<br>_In-domain supervised_ SimCSE_ (upper bound)_<br>65.2<br>88.3<br>88.11<br>84.46<br>83.63<br>81.94|65.2|88.3<br>88.11<br>84.46<br>83.63|88.3<br>88.11<br>84.46<br>83.63| Table 2: Evaluation results of the adapter-based domain adaptation using the different loss functions _ℓ_ 1 and _ℓ_ 2 . The evaluation metric is Mean Average Precision ( MAP ). We show the performance of the SimCSE model without domain-specific fine-tuning as a lower bound. Additionally, we show the performance of SimCSE models using traditional fine-tuning with the respective loss functions as upper bounds. For the upper bounds, all model weights have been updated during training. In contrast, only the adapter weights were updated during adapter training while the base model parameters were frozen. In bold, we highlight the best adapter performance overall and underline the best adapter results per loss function. tations of _x_ _i_, _x_ [+] _i_ [, and] _[ x]_ _[−]_ _i_ [, we use the triplet margin] loss function of Cohan et al. (2020) as follows: _ℓ_ 1 = max _{_ ( _d_ ( _h_ _i_ _, h_ [+] _i_ [)] _[ −]_ _[d]_ [(] _[h]_ _[i]_ _[, h]_ _[−]_ _i_ [) +] _[ m]_ [)] _[,]_ [ 0] _[}]_ [ (2)] where _d_ is the L2 norm distance function and _m_ is the loss margin hyperparameter set to 1. Additionally, we use the contrastive objective of Gao et al. (2021) as follows: _e_ _[sim]_ [(] _[hi,h]_ _i_ [+][)] _[/τ]_ _ℓ_ 2 = _−_ log ~~�~~ _Nj_ =1 [(] _[e]_ _sim_ ( _hi,h_ ~~[+]~~ _j_ [)] _[/τ]_ + _e_ _sim_ ( _hi,h_ ~~_[−]_~~ _j_ [)] _[/τ]_ ) (3) positive sample is defined as a directly referenced paper for each anchor sample. A negative sample is a paper referenced by the positive sample but not by the anchor sample itself. This approach ensures that all samples address the same topic, but the positive sample is more related to the anchor sample than the negative one.",
    "metadata": {
      "page_number": 3,
      "header": "Datasets",
      "chunk_index": 10,
      "file_path": "Efficient Domain Adaptation of Sentence Embeddings Using Adapters.pdf",
      "token_count": 3792
    }
  },
  {
    "text": "The AskUbuntu dataset (Lei et al., 2016) consists of user posts from the technical forum AskUbuntu. It already includes sentence pairs that are deemed similar. Therefore, anchor- and positive samples are easily found. Since the dataset inherently consists of sentences about a similar topic, the operating system Ubuntu, negative sentences can easily be retrieved by sampling different sentences. The dataset originates from a technical domain and is quite different from the scientific domain of SciDocs.",
    "metadata": {
      "page_number": 4,
      "header": "AskUbuntu",
      "chunk_index": 11,
      "file_path": "Efficient Domain Adaptation of Sentence Embeddings Using Adapters.pdf",
      "token_count": 505
    }
  },
  {
    "text": "Table 2 shows the results obtained when adapting sentence embedding models to different domains with adapters. To put the adapter results into perspective, we also evaluate the performance of the SimCSE base model, which is not adapted to the specific domains, as a lower bound. Furthermore, we use traditional domain-specific fine-tuning by training all parameters of the SimCSE base model with the respective loss functions as upper bounds with a mini-batch of _N_ triplets, a temperature hyperparameter _τ_, which is empirically set to 0.05, and _sim_ ( _h_ 1 _, h_ 2 ) as the cosine similarity _||hh_ 1 _||·||_ 1 _·hh_ 2 2 _||_ [.]",
    "metadata": {
      "page_number": 4,
      "header": "5 Evaluation",
      "chunk_index": 12,
      "file_path": "Efficient Domain Adaptation of Sentence Embeddings Using Adapters.pdf",
      "token_count": 635
    }
  },
  {
    "text": "The SciDocs dataset (Cohan et al., 2020) consists of scientific papers and their citation information. As model input, we concatenate the titles and abstracts of papers with the [SEP] token. Since our model has a maximum input length of 512 tokens, the input is cut off after this threshold. A The evaluation reveals that adapter-based domain adaptation yields competitive results compared to fine-tuning the entire base model. In particular, the Houlsby and Pfeiffer adapters perform very well with both loss functions, even though they use only a fraction of the parameters of the upper bounds. The slightly larger K-Adapter, however, performs considerably worse than the other adapters investigated. We conclude that the bottleneck architecture is more suitable than the ex ternal plug-in architecture for domain adaptation of sentence embedding models. In particular, the Houlsby adapter, although the smallest among the adapters investigated, yields the best results for both loss functions. Using the out-of-the-box SimCSE model without domain adaptation results in considerably worse performance, indicating the overall importance of domain-specific fine-tuning for sentence embedding models. Furthermore, the contrastive loss function _ℓ_ 2 performs consistently better than _ℓ_ 1 . Our results align with the observations of Gao et al. (2021) who conclude that the contrastive objective ensures a distribution of embeddings around the entire embedding space. In contrast, _ℓ_ 1 may yield learned representations occupying a narrow vector space cone, which severely limits their expressiveness. From the obtained results, we conclude that using the Houlsby-Adapter architecture together with the contrastive objective _ℓ_ 2 is most suitable for parameter-efficient domain adaptation of sentence embedding models. This adapter approach shows performance that is within 1% of the supervised, entirely fine-tuned SimCSE model, while only training approximately 3.6% of the parameters.",
    "metadata": {
      "page_number": 4,
      "header": "SciDocs",
      "chunk_index": 13,
      "file_path": "Efficient Domain Adaptation of Sentence Embeddings Using Adapters.pdf",
      "token_count": 1989
    }
  },
  {
    "text": "In this work, we proposed the use of adapters for parameter-efficient domain adaptation of sentence embedding models. In contrast to fine-tuning the entire sentence embedding model for a particular domain, adapters add a small number of new parameters that are updated during training while the weights of the sentence embedding model are fixed. We showed that adapter-based domain adaptation of sentence embedding models yields competitive results compared to fine-tuning the entire model, although only a fraction of the parameters are trained. In particular, we show that using the HoulsbyAdapter architecture together with a contrastive objective yields promising results for parameter efficient domain adaptation of sentence embedding models.",
    "metadata": {
      "page_number": 4,
      "header": "6 Conclusion",
      "chunk_index": 14,
      "file_path": "Efficient Domain Adaptation of Sentence Embeddings Using Adapters.pdf",
      "token_count": 747
    }
  },
  {
    "text": "Christos Baziotis, Mikel Artetxe, James Cross, and [Shruti Bhosale. 2022. Multilingual machine trans-](https://aclanthology.org/2022.emnlp-main.77) [lation with hyper-adapters. In](https://aclanthology.org/2022.emnlp-main.77) _Proceedings of the_ _2022 Conference on Empirical Methods in Natu-_ _ral Language Processing_, pages 1170–1185, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Samuel R. Bowman, Gabor Angeli, Christopher Potts, [and Christopher D. Manning. 2015. A large anno-](https://doi.org/10.18653/v1/D15-1075) [tated corpus for learning natural language inference](https://doi.org/10.18653/v1/D15-1075) In _Proceedings of the 2015 Conference on Empiri-_ _cal Methods in Natural Language Processing_, pages 632–642, Lisbon, Portugal. Association for Computational Linguistics. Rui Cao, Yihao Wang, Yuxin Liang, Ling Gao, Jie [Zheng, Jie Ren, and Zheng Wang. 2022. Explor-](https://doi.org/10.18653/v1/2022.findings-acl.248) [ing the impact of negative samples of contrastive](https://doi.org/10.18653/v1/2022.findings-acl.248) [learning: A case study of sentence embedding. In](https://doi.org/10.18653/v1/2022.findings-acl.248) _Findings of the Association for Computational Lin-_ _guistics: ACL 2022_, pages 3138–3152, Dublin, Ireland. Association for Computational Linguistics. Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St. John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, [Brian Strope, and Ray Kurzweil. 2018. Universal](https://doi.org/10.18653/v1/D18-2029) [sentence encoder for English.](https://doi.org/10.18653/v1/D18-2029) In _Proceedings of_ _the 2018 Conference on Empirical Methods in Nat-_ _ural Language Processing: System Demonstrations_, pages 169–174, Brussels, Belgium. Association for Computational Linguistics. Yung-Sung Chuang, Rumen Dangovski, Hongyin Luo, Yang Zhang, Shiyu Chang, Marin Soljacic, ShangWen Li, Scott Yih, Yoon Kim, and James Glass. 2022 [DiffCSE: Difference-based contrastive learning for](https://doi.org/10.18653/v1/2022.naacl-main.311) [sentence embeddings. In](https://doi.org/10.18653/v1/2022.naacl-main.311) _Proceedings of the 2022_ _Conference of the North American Chapter of the_ _Association for Computational Linguistics: Human_ _Language Technologies_, pages 4207–4218, Seattle, United States. Association for Computational Linguistics. Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, and Daniel Weld. 2020. [SPECTER:](https://doi.org/10.18653/v1/2020.acl-main.207) Document-level [representation](https://doi.org/10.18653/v1/2020.acl-main.207) learning using [citation-informed transformers.](https://doi.org/10.18653/v1/2020.acl-main.207) In _Proceedings_ _of the 58th Annual Meeting of the Association_ _for Computational Linguistics_, pages 2270–2282, Online. Association for Computational Linguistics. Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo ¨ ıc Barrault, and Antoine Bordes. 2017. [Supervised](https://doi.org/10.18653/v1/D17-1070) [learning of universal sentence representations from](https://doi.org/10.18653/v1/D17-1070) [natural language inference data. In](https://doi.org/10.18653/v1/D17-1070) _Proceedings of_ _the 2017 Conference on Empirical Methods in Nat-_ _ural Language Processing_, pages 670–680, Copenhagen, Denmark. Association for Computational Linguistics. Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. [SimCSE: Simple contrastive learning of sentence em-](https://doi.org/10.18653/v1/2021.emnlp-main.552) [beddings. In](https://doi.org/10.18653/v1/2021.emnlp-main.552) _Proceedings of the 2021 Conference_ _on Empirical Methods in Natural Language Process-_ _ing_, pages 6894–6910, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Ruidan He, Linlin Liu, Hai Ye, Qingyu Tan, Bosheng Ding, Liying Cheng, Jiawei Low, Lidong Bing, and [Luo Si. 2021. On the effectiveness of adapter-based](https://doi.org/10.18653/v1/2021.acl-long.172) [tuning for pretrained language model adaptation. In](https://doi.org/10.18653/v1/2021.acl-long.172) _Proceedings of the 59th Annual Meeting of the Asso-_ _ciation for Computational Linguistics and the 11th_ _International Joint Conference on Natural Language_ _Processing (Volume 1: Long Papers)_, pages 2208– 2222, Online. Association for Computational Linguistics. Felix Hill, Kyunghyun Cho, and Anna Korhonen. [2016. Learning distributed representations of sen-](https://doi.org/10.18653/v1/N16-1162) [tences from unlabelled data. In](https://doi.org/10.18653/v1/N16-1162) _Proceedings of the_ _2016 Conference of the North American Chapter of_ _the Association for Computational Linguistics: Hu-_ _man Language Technologies_, pages 1367–1377, San Diego, California. Association for Computational Linguistics. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. [Parameter-efficient transfer learning for NLP.](https://proceedings.mlr.press/v97/houlsby19a.html) In _Proceedings of the 36th International Conference_ _on Machine Learning_, volume 97 of _Proceedings_ _of Machine Learning Research_, pages 2790–2799. PMLR. [Yuxin Jiang, Linhan Zhang, and Wei Wang. 2022. Im-](https://aclanthology.org/2022.findings-emnlp.220) [proved universal sentence embeddings with prompt-](https://aclanthology.org/2022.findings-emnlp.220) [based contrastive learning and energy-based learning.](https://aclanthology.org/2022.findings-emnlp.220) In _Findings of the Association for Computational_ _Linguistics: EMNLP 2022_, pages 3021–3035, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Ryan Kiros, Yukun Zhu, Russ R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja [Fidler. 2015. Skip-thought vectors. In](https://proceedings.neurips.cc/paper/2015/file/f442d33fa06832082290ad8544a8da27-Paper.pdf) _Advances in_ _Neural Information Processing Systems_, volume 28. Curran Associates, Inc. Anne Lauscher, Olga Majewska, Leonardo F. R. Ribeiro, Iryna Gurevych, Nikolai Rozanov, and Goran Glavas. ˇ",
    "metadata": {
      "page_number": 4,
      "header": "References",
      "chunk_index": 15,
      "file_path": "Efficient Domain Adaptation of Sentence Embeddings Using Adapters.pdf",
      "token_count": 6097
    }
  },
  {
    "text": "[pretrained transformers.](https://doi.org/10.18653/v1/2020.deelio-1.5) In _Proceedings of Deep_ _Learning Inside Out (DeeLIO): The First Workshop_ _on Knowledge Extraction and Integration for Deep_ _Learning Architectures_, pages 43–49, Online. Association for Computational Linguistics. Hang Le, Juan Pino, Changhan Wang, Jiatao Gu, Didier [Schwab, and Laurent Besacier. 2021. Lightweight](https://doi.org/10.18653/v1/2021.acl-short.103) [adapter tuning for multilingual speech translation. In](https://doi.org/10.18653/v1/2021.acl-short.103) _Proceedings of the 59th Annual Meeting of the Asso-_ _ciation for Computational Linguistics and the 11th_ _International Joint Conference on Natural Language_ _Processing (Volume 2: Short Papers)_, pages 817–824, Online. Association for Computational Linguistics. Jaeseong Lee, Seung-won Hwang, and Taesup Kim",
    "metadata": {
      "page_number": 4,
      "header": "2020. [Common sense or world knowledge?](https://doi.org/10.18653/v1/2020.deelio-1.5) in[vestigating adapter-based knowledge injection into](https://doi.org/10.18653/v1/2020.deelio-1.5)",
      "chunk_index": 16,
      "file_path": "Efficient Domain Adaptation of Sentence Embeddings Using Adapters.pdf",
      "token_count": 855
    }
  },
  {
    "text": "[transfer to low-resource languages. In](https://aclanthology.org/2022.aacl-short.8) _Proceedings of_ _the 2nd Conference of the Asia-Pacific Chapter of the_ _Association for Computational Linguistics and the_ _12th International Joint Conference on Natural Lan-_ _guage Processing (Volume 2: Short Papers)_, pages 57–64, Online only. Association for Computational Linguistics. Tao Lei, Hrishikesh Joshi, Regina Barzilay, Tommi Jaakkola, Kateryna Tymoshenko, Alessandro Moschitti, and Llu ´ ıs Marquez. 2016. ` [Semi-supervised](https://doi.org/10.18653/v1/N16-1153) [question retrieval with gated convolutions. In](https://doi.org/10.18653/v1/N16-1153) _Pro-_ _ceedings of the 2016 Conference of the North Amer-_ _ican Chapter of the Association for Computational_ _Linguistics: Human Language Technologies_, pages 1279–1289, San Diego, California. Association for Computational Linguistics. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, ¨ Mike Lewis, Wen-tau Yih, Tim Rocktaschel, Sebastian Riedel, and Douwe ¨ Kiela. 2020. [Retrieval-augmented generation for](https://proceedings.neurips.cc/paper_files/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf) [knowledge-intensive nlp tasks.](https://proceedings.neurips.cc/paper_files/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf) In _Advances in_ _Neural Information Processing Systems_, volume 33, pages 9459–9474. Curran Associates, Inc. Che Liu, Rui Wang, Jinghua Liu, Jian Sun, Fei Huang, [and Luo Si. 2021. DialogueCSE: Dialogue-based](https://doi.org/10.18653/v1/2021.emnlp-main.185) [contrastive learning of sentence embeddings. In](https://doi.org/10.18653/v1/2021.emnlp-main.185) _Pro-_ _ceedings of the 2021 Conference on Empirical Meth-_ _ods in Natural Language Processing_, pages 2396– 2406, Online and Punta Cana, Dominican Republic Association for Computational Linguistics. [Lajanugen Logeswaran and Honglak Lee. 2018. An](https://openreview.net/forum?id=rJvJXZb0W) [efficient framework for learning sentence represen-](https://openreview.net/forum?id=rJvJXZb0W) [tations. In](https://openreview.net/forum?id=rJvJXZb0W) _International Conference on Learning_ _Representations_ . Sosuke Nishikawa, Ryokan Ri, Ikuya Yamada, Yoshimasa Tsuruoka, and Isao Echizen. 2022. [EASE:](https://doi.org/10.18653/v1/2022.naacl-main.284) [Entity-aware contrastive learning of sentence em-](https://doi.org/10.18653/v1/2022.naacl-main.284) [bedding. In](https://doi.org/10.18653/v1/2022.naacl-main.284) _Proceedings of the 2022 Conference_ _of the North American Chapter of the Association for_ _Computational Linguistics: Human Language Tech-_ _nologies_, pages 3870–3885, Seattle, United States Association for Computational Linguistics. Marinela Parovic, Goran Glava ´ s, Ivan Vuli ˇ c, and Anna ´ [Korhonen. 2022. BAD-X: Bilingual adapters im-](https://doi.org/10.18653/v1/2022.naacl-main.130) [prove zero-shot cross-lingual transfer. In](https://doi.org/10.18653/v1/2022.naacl-main.130) _Proceed-_ _ings of the 2022 Conference of the North Ameri-_ _can Chapter of the Association for Computational_ _Linguistics: Human Language Technologies_, pages 1791–1799, Seattle, United States. Association for Computational Linguistics. Jonas Pfeiffer, Gregor Geigle, Aishwarya Kamath, JanMartin Steitz, Stefan Roth, Ivan Vulic, and Iryna ´ [Gurevych. 2022. xGQA: Cross-lingual visual ques-](https://doi.org/10.18653/v1/2022.findings-acl.196) [tion answering. In](https://doi.org/10.18653/v1/2022.findings-acl.196) _Findings of the Association for_ _Computational Linguistics: ACL 2022_, pages 2497– 2511, Dublin, Ireland. Association for Computational Linguistics. Jonas Pfeiffer, Aishwarya Kamath, Andreas Ruckl ¨ e, ´ Kyunghyun Cho, and Iryna Gurevych. 2021a. [AdapterFusion: Non-destructive task composition](https://doi.org/10.18653/v1/2021.eacl-main.39) [for transfer learning. In](https://doi.org/10.18653/v1/2021.eacl-main.39) _Proceedings of the 16th Con-_ _ference of the European Chapter of the Association_ _for Computational Linguistics: Main Volume_, pages 487–503, Online. Association for Computational Linguistics. Jonas Pfeiffer, Andreas Ruckl ¨ e, Clifton Poth, Aishwarya ´ Kamath, Ivan Vulic, Sebastian Ruder, Kyunghyun ´ [Cho, and Iryna Gurevych. 2020a. AdapterHub: A](https://doi.org/10.18653/v1/2020.emnlp-demos.7) [framework for adapting transformers. In](https://doi.org/10.18653/v1/2020.emnlp-demos.7) _Proceedings_ _of the 2020 Conference on Empirical Methods in Nat-_ _ural Language Processing: System Demonstrations_, pages 46–54, Online. Association for Computational Linguistics. Jonas Pfeiffer, Ivan Vulic, Iryna Gurevych, and Se- ´ [bastian Ruder. 2020b. MAD-X: An Adapter-Based](https://doi.org/10.18653/v1/2020.emnlp-main.617) [Framework for Multi-Task Cross-Lingual Transfer.](https://doi.org/10.18653/v1/2020.emnlp-main.617) In _Proceedings of the 2020 Conference on Empirical_ _Methods in Natural Language Processing (EMNLP)_, pages 7654–7673, Online. Association for Computational Linguistics. Jonas Pfeiffer, Ivan Vulic, Iryna Gurevych, and Sebas- ´ [tian Ruder. 2021b. UNKs everywhere: Adapting](https://doi.org/10.18653/v1/2021.emnlp-main.800) [multilingual language models to new scripts. In](https://doi.org/10.18653/v1/2021.emnlp-main.800) _Pro-_ _ceedings of the 2021 Conference on Empirical Meth-_ _ods in Natural Language Processing_, pages 10186– 10203, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Nils Reimers and Iryna Gurevych. 2019. [Sentence-](https://doi.org/10.18653/v1/D19-1410) [BERT: Sentence embeddings using Siamese BERT-](https://doi.org/10.18653/v1/D19-1410) [networks. In](https://doi.org/10.18653/v1/D19-1410) _Proceedings of the 2019 Conference on_ _Empirical Methods in Natural Language Processing_ _and the 9th International Joint Conference on Natu-_ _ral Language Processing (EMNLP-IJCNLP)_, pages 3982–3992, Hong Kong, China. Association for Computational Linguistics. Phillip Schneider, Tim Schopf, Juraj Vladika, Mikhail Galkin, Elena Simperl, and Florian Matthes. 2022. [A decade of knowledge graphs in natural language](https://aclanthology.org/2022.aacl-main.46) [processing: A survey. In](https://aclanthology.org/2022.aacl-main.46) _Proceedings of the 2nd_ _Conference of the Asia-Pacific Chapter of the Asso-_ _ciation for Computational Linguistics and the 12th_ _International Joint Conference on Natural Language_ _Processing (Volume 1: Long Papers)_, pages 601–614, Online only. Association for Computational Linguistics. Tim Schopf, Karim Arabi, and Florian Matthes. 2023a [Exploring the landscape of natural language process-](http://arxiv.org/abs/2307.10652) [ing research.](http://arxiv.org/abs/2307.10652) Tim Schopf, Daniel Braun, and Florian Matthes. 2021 [Lbl2vec: An embedding-based approach for unsu-](https://doi.org/10.5220/0010710300003058) [pervised document retrieval on predefined topics. In](https://doi.org/10.5220/0010710300003058) _Proceedings of the 17th International Conference on_ _Web Information Systems and Technologies - WE-_ _BIST_, pages 124–132. INSTICC, SciTePress. Tim Schopf, Daniel Braun, and Florian Matthes. 2023b [Evaluating unsupervised text classification: Zero-](https://doi.org/10.1145/3582768.3582795) [shot and similarity-based approaches. In](https://doi.org/10.1145/3582768.3582795) _Proceed-_ _ings of the 2022 6th International Conference on_ _Natural Language Processing and Information Re-_ _trieval_, NLPIR ’22, page 6–15, New York, NY, USA Association for Computing Machinery. Tim Schopf, Daniel Braun, and Florian Matthes. 2023c [Semantic label representations with lbl2vec:](https://doi.org/https://doi.org/10.1007/978-3-031-24197-0_4) A [similarity-based approach for unsupervised text clas-](https://doi.org/https://doi.org/10.1007/978-3-031-24197-0_4) [sification. In](https://doi.org/https://doi.org/10.1007/978-3-031-24197-0_4) _Web Information Systems and Tech-_ _nologies_, pages 59–73, Cham. Springer International Publishing. Tim Schopf, Emanuel Gerber, Malte Ostendorff, and [Florian Matthes. 2023d. Aspectcse: Sentence em-](http://arxiv.org/abs/2307.07851) [beddings for aspect-based semantic textual similarity](http://arxiv.org/abs/2307.07851) [using contrastive learning and structured knowledge](http://arxiv.org/abs/2307.07851) Tim Schopf, Simon Klimek, and Florian Matthes. 2022 [Patternrank: Leveraging pretrained language models](https://doi.org/10.5220/0011546600003335) [and part of speech for unsupervised keyphrase extrac-](https://doi.org/10.5220/0011546600003335) [tion. In](https://doi.org/10.5220/0011546600003335) _Proceedings of the 14th International Joint_ _Conference on Knowledge Discovery, Knowledge En-_ _gineering and Knowledge Management (IC3K 2022)_ _- KDIR_, pages 243–248. INSTICC, SciTePress. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz [Kaiser, and Illia Polosukhin. 2017. Attention is all](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) [you need. In](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) _Advances in Neural Information Pro-_ _cessing Systems_, volume 30. Curran Associates, Inc. Marko Vidoni, Ivan Vulic, and Goran Glavas. 2020 [Orthogonal language and task adapters in zero-shot](http://arxiv.org/abs/2012.06460) [cross-lingual transfer.](http://arxiv.org/abs/2012.06460) _CoRR_, abs/2012.06460. Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Jianshu Ji, Guihong Cao, Daxin [Jiang, and Ming Zhou. 2021. K-Adapter: Infusing](https://doi.org/10.18653/v1/2021.findings-acl.121) [Knowledge into Pre-Trained Models with Adapters](https://doi.org/10.18653/v1/2021.findings-acl.121) In _Findings of the Association for Computational_ _Linguistics: ACL-IJCNLP 2021_, pages 1405–1418, Online. Association for Computational Linguistics. Adina Williams, Nikita Nangia, and Samuel Bowman [2018. A broad-coverage challenge corpus for sen-](https://doi.org/10.18653/v1/N18-1101) [tence understanding through inference. In](https://doi.org/10.18653/v1/N18-1101) _Proceed-_ _ings of the 2018 Conference of the North American_ _Chapter of the Association for Computational Lin-_ _guistics: Human Language Technologies, Volume_ _1 (Long Papers)_, pages 1112–1122, New Orleans, Louisiana. Association for Computational Linguistics. Xing Wu, Chaochen Gao, Liangjun Zang, Jizhong Han, [Zhongyuan Wang, and Songlin Hu. 2022. ESim-](https://aclanthology.org/2022.coling-1.342) [CSE: Enhanced sample building method for con-](https://aclanthology.org/2022.coling-1.342) [trastive learning of unsupervised sentence embed-](https://aclanthology.org/2022.coling-1.342) [ding. In](https://aclanthology.org/2022.coling-1.342) _Proceedings of the 29th International Con-_ _ference on Computational Linguistics_, pages 3898– 3907, Gyeongju, Republic of Korea. International Committee on Computational Linguistics. Miaoran Zhang, Marius Mosbach, David Adelani, Michael Hedderich, and Dietrich Klakow. 2022. [MCSE: Multimodal contrastive learning of sentence](https://doi.org/10.18653/v1/2022.naacl-main.436) [embeddings. In](https://doi.org/10.18653/v1/2022.naacl-main.436) _Proceedings of the 2022 Conference_ _of the North American Chapter of the Association for_ _Computational Linguistics: Human Language Tech-_ _nologies_, pages 5959–5969, Seattle, United States. Association for Computational Linguistics.",
    "metadata": {
      "page_number": 4,
      "header": "2022. [FAD-X: Fusing adapters for cross-lingual](https://aclanthology.org/2022.aacl-short.8)",
      "chunk_index": 17,
      "file_path": "Efficient Domain Adaptation of Sentence Embeddings Using Adapters.pdf",
      "token_count": 11525
    }
  }
]