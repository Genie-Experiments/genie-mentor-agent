## Chunk_1
```
NVIDIA

Vancouver, Canada eoldridge@nvidia.com king models play a crucial role in enhancing overall accuracy of retrieval systems. These multi-stage systems typically utilize er dense embedding models or sparse lexical indices to retrieve vant passages based on a given query, followed by ranking dels that refine the ordering of the candidate passages by its vance to the query. his paper benchmarks various publicly available ranking modand examines their impact on ranking accuracy. We focus on retrieval for question-answering tasks, a common use case for ieval-Augmented Generation systems. Our evaluation benchks include models some of which are commercially viable for ustrial applications. We introduce a state-of-the-art ranking model, _NV-RerankQA-_ _ral-4B-v3_, which achieves a significant accuracy increase of 14% pared to pipelines with other rerankers. We also provide an tion study comparing the fine-tuning of ranking models with rent sizes, losses and self-attention mechanisms. inally, we discuss challenges of text retrieval pipelines with king models in real-world industry applications, in particular trade-offs among model size, ranking accuracy and system uirements like indexing and serving latency / throughput.
```

## Chunk_2
```
Introduction

retrieval is a core component for many information retrieval ications like search, Question-Answering (Q&A) and recomder systems. More recently, text retrieval has been leveraged by ieval-Augmented Generation (RAG)[ 15, 27 ] systems, empowg Large Language Models (LLM) with external and up-to-date ext. ext embedding models represent variable-length text as a fixed ension vector that can be used for downstream tasks. They are for effective text retrieval, as they can semantically match pieces extual content that can be symmetric (e.g. similar sentences ocuments) or asymmetric (question and passages that might aining its answer). Embedding models are based on the Transformer architec Some examples of seminal works are Sentence-BERT [ 28 ], D [ 12 ], E5 [ 32 ] and E5-Mistral [ 33 ]. They are typically trained w Constrastive Learning [ 3, 12 ] as a _bi-encoder_ or _late combina_ _model_ [ 38 ], i.e. query and passage are embedded separately and model is optimized to maximize the similarity between query relevant (positive) passages and minimize the similarity betw query and non-relevant (negative) passages. Retrieval systems that leverage text embedding models typic split the corpus into small chunks or passages (e.g. sentence paragraphs), embed those passages and index corresponding em dings into a vector database. This setup allows efficiently retrie relevant passages from the embedded query by using Maxim Inner Product Search (MIPS) [ 15 ] or another Approximate Nea Neighbor (ANN) algorithm. The MTEB [ 22 ] is a popular benchmark of text embedding m els for different tasks like retrieval, classification, clustering mantic textual similarity, among others. We can notice from M leaderboard [1] that in general the larger the embedding mode terms of parameters the higher the accuracy it can achieve. H ever, that brings engineering challenges for companies in deplo such systems, as large embedding models can be prohibitively co or slow to index very large textual corpus / knowledge bases. For that reason, multi-stage text retrieval pipelines have b proposed to increase indexing and serving throughput, as we improving the retrieval accuracy. In those pipelines, a sparse an dense embedding model are first used to retrieve top-k candi passages, followed by a ranking model that refines the final ran of those passages, as illustrated in Figure 1. Ranking models are typically Transformer models that ope as a _cross-encoder_ or _early combination model_ [ 38 ] that take input both the query and passage pair concatenated and uses self-attention mechanism to interact more deeply with the qu and passage pair, and model their semantic relationship. Rank models are used to provide relevance predictions only for the k candidates retrieved by the retrieval model. They can incr retrieval accuracy and make it possible using smaller embedd model, considerably reducing the indexing time and cost. In this paper, we present a benchmark of publicly available r ing models for text retrieval and discuss how they affect the ran accuracy compared to the original ordering provided by diffe 1 [https://huggingface.co/spaces/mteb/leaderboard](https://huggingface.co/spaces/mteb/leaderboard)
```

## Chunk_3
```
elines for multi-stage text retrieval

edding models. To ensure the usefulness of this benchmark for panies, in our experimentation we include certain embedding ranking models that can be used commercially, i.e. that have a per license and were not trained on research-only public data h as the popular MS-MARCO [1] dataset. he main contributions of this paper are: - We provide a comprehensive accuracy evaluation of publicly available ranking models with different commercially usable embedding models for Q&A Text Retrieval; - We introduce a state-of-the-art ranking model: _NV-RerankQA-_ _Mistral-4B-v3_, and describe its architecture, pruning from Mistral 7B and fine-tuning techniques for leading ranking accuracy; - We present an ablation study fine-tuning ranking models on top of different base model sizes. Then for the Mistral base model, we experiment with both point-wise and pairwise losses and different attention/pooling mechanisms and discuss how these combinations affect the accuracy; - Finally, we discuss trade-off aspects related to deploying text retrieval pipelines with or without a ranking model, like inference latency and embeddings indexing throughput.
```

## Chunk_4
```
Background and Related work

arlier days of information retrieval, a common approach to ne search results based on sparse retrieval models (e.g. BM25) to use feature-based learning-to-rank models with point-wise, -wise or list-wise losses [16]. eural Ranking Models (NRM) [ 38 ], typically feed-forward netks, were proposed to capture interactions between query and ument. DeepMatch [ 17 ] modeled each text as a sequence of ms to generate a matching score. DRMM [ 8 ] represents the input s as histogram-based features. Duet [ 20 ] proposed a ranking del composed of two separate neural networks trained jointly, that matches query and document using local representation and another using learned distributed representation. In [ 6 put text was represented with bag-of-words and averaged ba embeddings, and it was proposed three point-wise and pair-w models, trained using weekly supervised data. Transformers [ 30 ] have moved natural language proces (NLP) field from manually handcrafting features to learning mantic and deeper text representations. The Deep Learning T at TREC 2019 [ 5 ] hosted an extensive assessment of retrieval m els after BERT [ 7 ] was introduced by Google, and demonstr the effectiveness of leveraging pre-trained transformers as ran models [9]. In [ 23 ] BERT was adapted as a passage re-ranker. They prop leveraging the _next sentence prediction_ training task to feed as in the concatenated query and passage separated by a special to and adding on top of the [CLS] output vector a single layer bin classification model. The authors later proposed in [ 24 ] a m stage retrieval pipeline composed by BM25 and two BERT ran models: one trained with point-wise loss (monoBERT) and the o with a pair-wise loss (duoBERT). To deal with BERT limitation of 512 tokens, [ 36 ] proposed s ting documents into sentences for BERT usage. In [ 26 ] they ex ment with bi-encoder and cross-encoder ranking models base BERT, the latter being more effective due to the deeper interac it provides. They also noticed BERT is more effective when wor with semantically close query-document pairs compared to u click data for training. Since then, a number of _cross-encoder_ models based on Tr formers have been released from the community [2 3] . We dis and compare some of the most accurate ranking models avail publicly in the next section.
```

## Chunk_5
```
3 Benchmarking ranking models for Q&A te retrieval

Benchmarking models in terms of accuracy is important to sup decision on which models should be used in production pipel or fine-tuned for domain adaptation. In this section, we evaluate text retrieval pipelines compose different embedding and ranking models. To ensure the useful of this benchmark for companies, we evaluate the pipelines u three commercially usable embedding models and top-perform ranking models. We emphasize our evaluation on Question-Answering (Q datasets, as that is a popular application of RAG systems.
```

## Chunk_6
```
3.1 Retrieval models

There are many embedding models publicly available for the c munity, whose accuracy for multiple tasks can be found at MTEB leaderboard [4] . Most of those models have being traine research-only datasets like MS-MARCO and cannot be used c mercially. We evaluate embedding models that can be used for industry retrieval applications. We select for experiments three embed 2 [https://huggingface.co/cross-encoder](https://huggingface.co/cross-encoder) 3 [https://www.sbert.net/docs/cross_encoder/pretrained_models.html](https://www.sbert.net/docs/cross_encoder/pretrained_models.html) 4 [https://huggingface.co/spaces/mteb/leaderboard](https://huggingface.co/spaces/mteb/leaderboard) els for candidate retrieval, as the emphasis of our experiments valuating ranking models: - _Snowflake/snowflake-arctic-embed-l_ [ 19 ] [5 6] (335M params) - Embedding model based on BERT-large, trained with contrastive learning for two training rounds, with in-batch negative samples and hard negatives. - _nvidia/nv-embedqa-e5-v5_ [7] (335M params) - Embedding model based on _e5-large-unsupervised_ trained for multiple rounds on supervised data with contrastive learning, and both in-batch negatives and hard-negatives. - _nvidia/nv-embedqa-mistral-7b-v2_ [8] (7.24B params)- Large embedding model based on Mistral 7B v0.1 [ 11 ] [9], modified to use bi-directional attention and average pooling as done in NV-Embed [14] and NV-Retriever [21].
```

## Chunk_7
```
Ranking models

describe here the ranking models we evaluated in this investigaincluding recent ranking models that perform high on retrieval chmarks according to their reports. From those, the only rerankmodels that can be used for commercial purposes from their nces and train data are _NV-RerankQA-Mistral-4B-v3_, which we oduce in this paper, and _mixedbread-ai/mxbai-rerank-large-v1_ . - _ms-marco-MiniLM-L-12-v2_ [10] (33M params) - fine-tuned on top of the MiniLMv2 model [ 34 ] with _SentenceTransformers_ package [11] on the MS MARCO passage ranking dataset [ 1 ]. - _jina-reranker-v2-base-multilingual_ [12] (278M params) - A multi-lingual ranking model finetuned from XLM-RoBERTa [4] - _mixedbread-ai/mxbai-rerank-large-v1_ [13] (435M params) Largest re-ranker model provided by Mixedbread - _bge-reranker-v2-m3_ [14] (568M params) - A multi-lingual ranking model fine-tuned from _BGE M3-Embedding_ [ 2 ] with FlagEmbedding package [15] - _NV-RerankQA-Mistral-4B-v3_ [16] (4B params) - Large and powerful re-ranker that takes as base model a pruned version of Mistral 7B and is fine-tuned with a blend of supervised data with contrastive learning. It is fully described in Section 4.
```

## Chunk_8
```
Evaluation setup

evaluation setup mimics the typical text retrieval indexing and rying pipelines, as previously illustrated in Figure 1. he indexing pipeline takes place first, where a text corpus is nked into smaller passages. For our evaluation, we use datasets m BEIR [ 29 ] datasets, which are already chunked and truncated max 512 tokens. The chunked passages are embedded using an [s://huggingface.co/Snowflake/snowflake-arctic-embed-l](https://huggingface.co/Snowflake/snowflake-arctic-embed-l) [s://build.nvidia.com/snowflake/arctic-embed-l](https://build.nvidia.com/snowflake/arctic-embed-l) [s://build.nvidia.com/nvidia/nv-embedqa-e5-v5](https://build.nvidia.com/nvidia/nv-embedqa-e5-v5) [s://build.nvidia.com/nvidia/nv-embedqa-mistral-7b-v2](https://build.nvidia.com/nvidia/nv-embedqa-mistral-7b-v2) [s://huggingface.co/mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) [ps://huggingface.co/cross-encoder/ms-marco-MiniLM-L-12-v2](https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-12-v2) [ps://www.sbert.net/](https://www.sbert.net/) [ps://huggingface.co/jinaai/jina-reranker-v2-base-multilingual](https://huggingface.co/jinaai/jina-reranker-v2-base-multilingual) [ps://huggingface.co/mixedbread-ai/mxbai-rerank-large-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-large-v1) [ps://huggingface.co/BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) [ps://github.com/FlagOpen/FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding) [ps://build.nvidia.com/explore/retrieval#nv-rerankqa-mistral-4b-v3](https://build.nvidia.com/explore/retrieval#nv-rerankqa-mistral-4b-v3) embedding model and stored in a vector index / database. The qu ing pipeline then takes place for providing for each query a list w ranked passages for retrieval metrics computation (NDCG@ In detail, the question is embedded and it is performed a ve search (e.g. using exact or Approximate Nearest Neighbour (A algorithm) on the vector index, returning the top-k most relev passages for the question. Finally, the top-k (set to 100 in our ev ation experiments) passages are re-ranked with a ranking mod generate the final ordered list. We perform the evaluation on the three Question-Answe datasets from BEIR [ 29 ] retrieval benchmark: Natural Quest (NQ) [13], HotpotQA [37] and FiQA [18].
```

## Chunk_9
```
3.4 Benchmark results

In this section, we provide the benchmark results of text retri pipelines with different embedding and ranking models. In Table 1, we compare pipelines with three commercially us embedding models (Section 3.1) and their combination with a n ber of ranking models (Section 3.2). Retrieval accuracy is meas with NDCG@10 for Q&A BEIR datasets.
```

## Chunk_10
```
Reranker model Avg.** **NQ** **HotpotQA

_**Embedding:**_ **snowflake-arctic-embed-l** 0.6100 0.6311 0.7518 + ms-marco-MiniLM-L-12-v2 0.5771 0.5876 0.7586 + mxbai-rerank-large-v1 0.6077 0.6433 0.7401 + jina-reranker-v2-base-multilingual 0.6481 0.6768 0.8165 + bge-reranker-v2-m3 0.6585 0.6965 0.8458 + NV-RerankQA-Mistral-4B-v3 **0.7529** **0.7788** **0.8726** _**Embedding:**_ **NV-EmbedQA-e5-v5** 0.6083 0.6380 0.7160 + ms-marco-MiniLM-L-12-v2 0.5785 0.5909 0.7458 + mxbai-rerank-large-v1 0.6077 0.6450 0.7279 + jina-reranker-v2-base-multilingual 0.6454 0.6780 0.7996 + bge-reranker-v2-m3 0.6584 0.6974 0.8272 + NV-RerankQA-Mistral-4B-v3 **0.7486** **0.7785** **0.8470** _**Embedding:**_ **NV-EmbedQA-Mistral7B-v2** 0.7173 0.7216 0.8109 + ms-marco-MiniLM-L-12-v2 0.5875 0.5945 0.7641 + mxbai-rerank-large-v1 0.6133 0.6439 0.7436 + jina-reranker-v2-base-multilingual 0.6590 0.6819 0.8262 + bge-reranker-v2-m3 0.6734 0.7028 0.8635 + NV-RerankQA-Mistral-4B-v3 **0.7694** **0.7830** **0.8904** We can clearly observe that for smaller embedding models _snowflake-arctic-embed-l_ and _NV-EmbedQA-e5-v5_ (335M para all cross-encoders (except for the small _ms-marco-MiniLM-L_ _v2_ ) improve considerably the ranking accuracy compared to retriever. On the other hand, for the larger _NV-EmbedQA-Mistra_ _v2_ embedding model, only the large _NV-RerankQA-Mistral-4B_ reranker is able to improve its accuracy. The _NV-RerankQA-Mistral-4B-v3_ reranker provides the hig ranking accuracy for all datasets by a large margin (+14% comp to the second best reranker: _bge-reranker-v2-m3_ ). That demonstr the effectiveness of our adaptation of Mistral 7B as a cross-enc
```

## Chunk_11
```
3.5 A small note about model licensing

For training _NV-RerankQA-Mistral-4B-v3_ we have selected public datasets whose license allows their usage for industry a cations. Some other models are released with permissive lice Apache 2.0 or MIT, but we do not know which datasets they e trained on or whether they got a special license to use researchy datasets like MS-Marco, for example. Every company should k with its legal team on model licensing for commercial usage.
```

## Chunk_12
```
Fine-tuning a state-of-the-art ranking model:

_**NV-RerankQA-Mistral-4B-v3**_ ntroduce in this paper the state-of-the-art _NV-RerankQA-Mistral-_ _v3_, that performs best in our benchmark on text retrieval for A (Section 3.4). Mistral 7B [ 11 ] decoder model has been successfully adopted as edding models for retrieval when repurposed and fine-tuned h contrastive learning[14, 21, 33]. n this work, we adapted Mistral 7B v0.1 [ 11 ] as a ranking model. rder to reduce the number of parameters from the base model, its inference compute and memory requirements, we prune it eeping only the bottom 16 layers out of its 32 layers [17] . We also ify its self-attention mechanism from uni-directional (causal) to irectional, so that for each token it is possible to attend to other ns in both right and left sides, as that has shown to improve uracy for Mistral-based embedding models [14, 21]. We feed as input to the model the tokenized question and candipassage pair, concatenated and separated by a special token. perform average pooling on the outputs of last Transformer r and add a feed-forward layer on top that outputs a single-unit the likelihood of a given passage being relevant to a question.
```

## Chunk_13
```
oder, pruned and adapted from Mistral 7B

ross-encoder ranking models are binary classifiers that discrime between positive and negative passages. They typically are ned with the binary cross-entropy loss as in Equation 1, where _𝜙_ ( _𝑞,𝑑_ ) is the model predicted likelihood of the passage _𝑑_ being vant to query _𝑞_ . _𝐿_ = −( _𝑦_ log( _𝑝_ ) + (1 − _𝑦_ ) log(1 − _𝑝_ )) (1) also tried pruning different number of top layers from Mistral 7B model, the layers we remove the lower the accuracy. We found out that keeping bottom yers provides a good trade-off between accuracy penalty (-1%) and model size tion (-50% # parameters) compared to the original 32-layer model. Instead, for _NV-RerankQA-Mistral-4B-v3_ we follow [ 31 ] and t the reranker with contrastive learning over the positive an negative candidates scores using the list-wise InfoNCE loss shown in Equation 2, where _𝑑_ [+] is a positive relevant passage, _𝑑_ one of the _𝑁_ negative passages and _𝜏_ is the temperature param _𝐿_ = −log exp( _𝜙_ ( _𝑞,𝑑_ [+] )/ _𝜏_ )) exp( _𝜙_ ( _𝑞,𝑑_ [+] )/ _𝜏_ )) + ~~[�]~~ _𝑖_ _[𝑁]_ =1 [exp][(] _[𝜙]_ [(] _[𝑞,𝑑]_ [−] [)/] _[𝜏]_ [))] The negative candidates used for contrastive learning are m from the corpus in the data pre-processing stage by using a tea embedding model. We use the _TopK-PercPos_ hard-negative min method introduced in [ 21 ], configured with maximum nega score threshold as 95% of the positive scores to remove poten false negatives. We present in Section 5 an ablation study on fine-tuning ran models, with some experiments focused on our choices for the and self-attention mechanism for _NV-RerankQA-Mistral-4B-v3_
```

## Chunk_14
```
5 Ablation study on fine-tuning ranking mod

In this section, we present an ablation study on fine-tuning comparing different base models as rerankers. For Mistral b model, we also compare choices of self-attention mechanism (u rectional vs bi-directional) and training losses (binary vs catego cross-entropy). For broader comparison, we evaluate the ranking models w the three different embedding models described in Section 3.1
```

## Chunk_15
```
5.1 Model size matters

Model size is an important aspect for trading-off model accuracy inference throughput. For this section we fine-tune and comp three base models with different sizes as ranking models: _Min_ _L12-H384-uncased_ [ 35 ] (33M params) [18], _deberta-v3-large_ [ 10 (435M) and _Mistral 4B_ (4B params), the latter pruned and mod from Mistral 7B v0.1 [11] as described in Section 4. Those ranking models are all fine-tuned with the same com budget (max 4 hours of training in a single server with 8x A GPUs) and same train set. In Table 2, we present the comparison of fine-tuning those ferent ranking models, with the same train set and compute bu Although the pre-training of those base models is different, possible to observe a pattern where larger ranking models pro higher retrieval accuracy. The most accurate model is based on _Mistral 4B_, but _deb_ _v3-large_ is surprisingly accurate for its smaller number of para ters, and is a good candidate architecture for deploying as a c encoder, as we discuss in Section 6.
```

## Chunk_16
```
5.2 Causal vs Bi-directional Attention mechanism

In Section 4 we describe that for our adapted _Mistral 4B_ we m ified the standard self-attention mechanism of Mistral from directional (causal) to bi-directional attention. 18 [https://huggingface.co/microsoft/MiniLM-L12-H384-uncased](https://huggingface.co/microsoft/MiniLM-L12-H384-uncased) 19 [https://huggingface.co/microsoft/deberta-v3-large](https://huggingface.co/microsoft/deberta-v3-large)
```

## Chunk_17
```
ranker model Avg.** **NQ** **HotpotQA** **FiQA

_**bedding:**_ **snowflake-arctic-embed-l** 0.6100 0.6311 0.7518 0.4471 iniLM-L12-H384-uncased 0.6227 0.6436 0.8128 0.4118 berta-v3-large 0.7277 0.7452 0.8548 0.5832 istral 4B **0.7414** **0.7690** **0.8681** **0.5872** _**bedding:**_ **NV-EmbedQA-e5-v5** 0.6083 0.6380 0.7160 0.4710 iniLM-L12-H384-uncased 0.6213 0.6438 0.7954 0.4248 berta-v3-large 0.7150 0.7441 0.8319 0.5689 istral 4B **0.7366** **0.7689** **0.8423** **0.5987** _**bedding:**_ **NV-EmbedQA-Mistral7B-v2** 0.7173 0.7216 0.8109 0.6194 iniLM-L12-H384-uncased 0.6355 0.6484 0.8269 0.4312 berta-v3-large 0.7413 0.7486 0.8700 0.6055 istral 4B **0.7575** **0.7717** **0.8857** **0.6152** We compare the accuracy with those two self-attention mechms in Table 3, both using average pooling, and demonstrate effectiveness of bi-directional attention for allowing deeper raction among input query and passage tokens.
```

## Chunk_18
```
ranker model Avg.** **NQ** **HotpotQA** **FiQA

_**bedding:**_ **snowflake-arctic-embed-l** 0.6100 0.6311 0.7518 0.4471 istral 4B (unidirectional attention) 0.7312 0.7663 0.8612 0.5660 istral 4B (bidirectional attention) **0.7414** **0.7690** **0.8681** **0.5872** _**bedding:**_ **NV-EmbedQA-e5-v5** 0.6083 0.6380 0.7160 0.4710 istral 4B (unidirectional attention) 0.7264 0.7655 0.8372 0.5766 istral 4B (bidirectional attention) **0.7366** **0.7689** **0.8423** **0.5987** _**bedding:**_ **NV-EmbedQA-Mistral7B-v2** 0.7173 0.7216 0.8109 0.6194 istral 4B (unidirectional attention) 0.7464 0.7690 0.8781 0.5920 istral 4B (bidirectional attention) **0.7575** **0.7717** **0.8857** **0.6152**
```

## Chunk_19
```
BCE vs InfoNCE Loss

ss-encoders are typically trained with the point-wise Binary ss-Entropy (BCE) loss (Equation 1), as we discussed in Section 4. n the other hand, we fine-tune _Mistral 4B_ with the list-wise NCE loss[25] (Equation 2) and contrastive learning. We experiment with those two losses, both using the same sets ard-negative passages mined from the corpus, as described in ion 4. n Table 4, we can clearly observe the higher retrieval accuracy ined when using InfoNCE, a list-wise contrastive learning loss ned to maximize the relevance score of the question and positive age pair, while minimizing the score for question and negative age pairs. his ablation study explains our choices of using bi-directional ntion and InfoNCE loss for fine-tuning _NV-RerankQA-Mistral-_ _3_ .
```

## Chunk_20
```
Reranker model Avg.** **NQ** **HotpotQA

_**Embedding:**_ **snowflake-arctic-embed-l** 0.6100 0.6311 0.7518 + Mistral 4B (BCE loss) 0.7230 0.7375 0.8609 + Mistral 4B (InfoNCE loss) **0.7414** **0.7690** **0.8681** _**Embedding:**_ **NV-EmbedQA-e5-v5** 0.6083 0.6380 0.7160 + Mistral 4B (BCE loss) 0.7171 0.7368 0.8357 + Mistral 4B (InfoNCE loss) **0.7366** **0.7689** **0.8423** _**Embedding:**_ **NV-EmbedQA-Mistral7B-v2** 0.7173 0.7216 0.8109 + Mistral 4B (BCE loss) 0.7373 0.7394 0.8774 + Mistral 4B (InfoNCE loss) **0.7575** **0.7717** **0.8857** embedding model would be computationally expensive, espec if the document corpus is large. Furthermore, when we de query pipeline to production, it is critical that it can handle a l number of queries in a timely manner and scale on demand. In s cases, it might be possible to improve both the retrieval accu and indexing throughput by replacing a single-stage query pipe of a large embedding model by a two-stage pipeline composed smaller embedding model and a ranking model. We conduct performance experiments with our models mized [20] with TensorRT [21] and deployed on Triton Inference Serv In Table 5, we present the average query embedding latency passages indexing throughput for two embedding models with ferent sizes. Although the time difference to embed a single qu with those two models will not compromise the overall onlin trieval latency, the indexing time of the chunked passages of textual corpus will take 8.2x longer with the larger embedd model, which results in higher compute/cost requirements whe indexing is needed (e.g. when corpus or embedding model chan
```

## Chunk_21
```
Embedding model Query embedding** **Passages** **indexi latency throughput

NV-EmbedQA-E5-v5 5.1 ms 558.4 passages/s NV-EmbedQA-Mistral7B-v2 19.8 ms 68.7 passages/s Thus, by using a two-stage pipeline with _NV-EmbedQA-E5-v5_ bedding and _NV-RerankQA-Mistral-4B-v3_ ranking models instea a single-stage pipeline with _NV-EmbedQA-Mistral7B-v2_, in addi to achieving higher retrieval accuracy (see Table 1), we will reduce the indexing time by 8.2x. We also have to consider the latency the ranking model add the query pipeline. In our example, after the query is embed with _NV-EmbedQA-E5-v5_ (in 5.1 ms), candidate relevant pass 20 You can find Nemo Retriever embedding and ranking models optimized fo [serving with TensorRT and Triton Inference Server in https://docs.nvidia.com](https://docs.nvidia.com/nim/index.html#nemo-retriever) [index.html#nemo-retriever](https://docs.nvidia.com/nim/index.html#nemo-retriever) 21 [https://developer.nvidia.com/tensorrt](https://developer.nvidia.com/tensorrt) 22 [https://developer.nvidia.com/triton-inference-server](https://developer.nvidia.com/triton-inference-server) 23 You can find more information on Nemo Retriever performance benchmarks [other GPUs / batch sizes for embedding models in https://docs.nvidia.com/nim/n](https://docs.nvidia.com/nim/nemo-retriever/text-embedding/latest/performance.html) [retriever/text-embedding/latest/performance.html and for ranking models in](https://docs.nvidia.com/nim/nemo-retriever/text-embedding/latest/performance.html) [//docs.nvidia.com/nim/nemo-retriever/text-reranking/latest/performance.html](https://docs.nvidia.com/nim/nemo-retriever/text-reranking/latest/performance.html) retrieved from a vector database using ANN, and the top-40 ages will be provided to a ranking model for final ranking. The andidate passages would be scored on their relevancy with ect to the query by the _NV-RerankQA-Mistral-4B-v3_ model in l 266 ms on average, which might add a reasonable time to the rall query pipeline latency depending on the non-functional irements for the system. That could be improved by distributing ranking scoring requests of those 40 candidates among multiple Us / Triton Inference Server instances. Another option would eploying a model with a good trade-off between size/latency accuracy, like _deberta-v3-large_ [24], which is much smaller (435M) n _NV-RerankQA-Mistral-4B-v3_ (4B) and might provide a good king accuracy as discussed in the ablation study on Table 2. n summary, the decision of which models to include in a text eval pipeline should consider the business requirements for eval accuracy and system requirements on indexing throughput serving latency. For example, if the text corpus to index is huge, bably indexing throughput will be the bottleneck and smaller edding models should be used. On the other hand, if serving ncy requirements are very strict, a fast query pipeline is more cal, and large ranking models should be avoided.
```

## Chunk_22
```
Conclusion

his paper, we provide a comprehensive evaluation of multi-stage retrieval pipelines for Question-Answering, a common use case RAG applications. The evaluated pipelines composed of comcially viable embedding models and state-of-the-art ranking dels. We introduce the _NV-RerankQA-Mistral-4B-v3_, that pros the best ranking accuracy by a large margin in our benchmark is commercially usable for industrial applications. We describe how we adapted the decoder-only _Mistral 7B_ and -tuned it as a cross-encoder, pruning the base model and modifyts attention and pooling mechanism to build the _NV-RerankQA-_ _ral-4B-v3_ . We also provide an ablation study comparing the fine-tuning ifferent-sized base models as cross-encoders, and highlight relationship between their number of parameters and ranking racy. We also compare the benefits of leveraging bi-directional ntion and InfoNCE loss for training a Mistral cross-encoder. inally, we discussed important deployment considerations for world text retrieval systems, with respect to trading-off model retrieval accuracy and systems requirements like serving lay and indexing throughput.
```

## Chunk_23
```
2016. Ms marco: A human generated machine reading comprehension dataset.

_arXiv preprint arXiv:1611.09268_ (2016). Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2024. Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation. _arXiv preprint arXiv:2402.03216_ (2024). Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A simple framework for contrastive learning of visual representations. In _Inter-_ _national conference on machine learning_ . PMLR, 1597–1607. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, are actually building a ranking model based on _deberta-v3-large_ and plan to se it soon. and Veselin Stoyanov. 2019. Unsupervised cross-lingual representation lea at scale. _arXiv preprint arXiv:1911.02116_ (2019). [5] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ell Voorhees. 2020. Overview of the TREC 2019 deep learning track. _arXiv pr_ _arXiv:2003.07820_ (2020). [6] Mostafa Dehghani, Hamed Zamani, Aliaksei Severyn, Jaap Kamps, and W Croft. 2017. Neural ranking models with weak supervision. In _Proceedin_ _the 40th international ACM SIGIR conference on research and developme_ _information retrieval_ . 65–74. [7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Pre-training of deep bidirectional transformers for language understan _arXiv preprint arXiv:1810.04805_ (2018). [8] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Croft. 2016. A deep rele matching model for ad-hoc retrieval. In _Proceedings of the 25th ACM interna_ _on conference on information and knowledge management_ . 55–64. [9] Kailash A. Hambarde and Hugo Proença. 2023. Information Retrieval: R Advances and Beyond. _IEEE Access_ [11 (2023), 76581–76604. https://doi.o](https://doi.org/10.1109/ACCESS.2023.3295776) [1109/ACCESS.2023.3295776](https://doi.org/10.1109/ACCESS.2023.3295776) [10] Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2021. Debertav3: Impr deberta using electra-style pre-training with gradient-disentangled embe sharing. _arXiv preprint arXiv:2111.09543_ (2021). [11] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford vendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Len Guillaume Lample, Lucile Saulnier, et al . 2023. Mistral 7B. _arXiv pr_ _arXiv:2310.06825_ (2023). [12] Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, S Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for domain question answering. _arXiv preprint arXiv:2004.04906_ (2020). [13] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, A Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ke Lee, et al . 2019. Natural questions: a benchmark for question answering res _Transactions of the Association for Computational Linguistics_ 7 (2019), 453– [14] Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Sh Bryan Catanzaro, and Wei Ping. 2024. NV-Embed: Improved Techniqu Training LLMs as Generalist Embedding Models. _arXiv preprint arXiv:2405._ (2024). [15] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vla Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim täschel, et al . 2020. Retrieval-augmented generation for knowledge-intensiv tasks. _Advances in Neural Information Processing Systems_ 33 (2020), 9459– [16] Tie-Yan Liu et al. 2009. Learning to rank for information retrieval. _Found_ _and Trends® in Information Retrieval_ 3, 3 (2009), 225–331. [17] Zhengdong Lu and Hang Li. 2013. A deep architecture for matching short _Advances in neural information processing systems_ 26 (2013). [18] Macedo Maia, Siegfried Handschuh, André Freitas, Brian Davis, Ross McDer Manel Zarrouk, and Alexandra Balahur. 2018. Www’18 open challenge: fina opinion mining and question answering. In _Companion proceedings of th_ _web conference 2018_ . 1941–1942. [19] Luke Merrick, Danmei Xu, Gaurav Nuti, and Daniel Campos. 2024. A Embed: Scalable, Efficient, and Accurate Text Embedding Models. _arXiv pr_ _arXiv:2405.05374_ (2024). [20] Bhaskar Mitra, Fernando Diaz, and Nick Craswell. 2017. Learning to match local and distributed representations of text for web search. In _Proceedings_ _26th international conference on world wide web_ . 1291–1299. [21] Gabriel de Souza P Moreira, Radek Osmulski, Mengyao Xu, Ronay Ak, Ben Schifferer, and Even Oldridge. 2024. NV-Retriever: Improving text embed models with effective hard-negative mining. _arXiv preprint arXiv:2407._ (2024). [22] Niklas Muennighoff, Nouamane Tazi, Loïc Magne, and Nils Reimers. 2022. M Massive text embedding benchmark. _arXiv preprint arXiv:2210.07316_ (202 [23] Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage Re-ranking with B _arXiv preprint arXiv:1901.04085_ (2019). [24] Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, and Jimmy Lin. 2019. Multidocument ranking with BERT. _arXiv preprint arXiv:1910.14424_ (2019). [25] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation lea with contrastive predictive coding. _arXiv preprint arXiv:1807.03748_ (2018) [26] Yifan Qiao, Chenyan Xiong, Zhenghao Liu, and Zhiyuan Liu. 2019. U standing the Behaviors of BERT in Ranking. _arXiv preprint arXiv:1904._ (2019). [27] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, K Leyton-Brown, and Yoav Shoham. 2023. In-context retrieval-augmented lang models. _Transactions of the Association for Computational Linguistics_ 11 ( 1316–1331. [28] Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embed using siamese bert-networks. _arXiv preprint arXiv:1908.10084_ (2019). [29] Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Gurevych. 2021. Beir: A heterogenous benchmark for zero-shot evaluati information retrieval models. _arXiv preprint arXiv:2104.08663_ (2021). Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. _Advances in neural information processing systems_ 30 (2017). Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022. Simlm: Pre-training with representation bottleneck for dense passage retrieval. _arXiv preprint arXiv:2207.02578_ (2022). Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022. Text embeddings by weakly-supervised contrastive pre-training. _arXiv preprint arXiv:2212.03533_ (2022). Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2023. Improving text embeddings with large language models. _arXiv_ _preprint arXiv:2401.00368_ (2023). Wenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong, and Furu Wei. 2020. Minilmv2: Multi-head self-attention relation distillation for compressing pretrained transformers. _arXiv preprint arXiv:2012.15828_ (2020). [35] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming
```

## Chunk_24
```
2020. Minilm: Deep self-attention distillation for task-agnostic compressi

pre-trained transformers. _Advances in Neural Information Processing Syste_ (2020), 5776–5788. [36] Wei Yang, Haotian Zhang, and Jimmy Lin. 1903. Simple applications of B for ad hoc document retrieval. CoRR abs/1903.10972 (2019). _URL: http://_ _org/abs/1903.10972_ (1903). [37] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, R Salakhutdinov, and Christopher D Manning. 2018. HotpotQA: A dataset f verse, explainable multi-hop question answering. _arXiv preprint arXiv:1809._ (2018). [38] Hamed Zamani, Mostafa Dehghani, W Bruce Croft, Erik Learned-Miller Jaap Kamps. 2018. From neural re-ranking to neural ranking: Learning a s representation for inverted indexing. In _Proceedings of the 27th ACM interna_ _conference on information and knowledge management_ . 497–506.
```

